{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция №4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Производная в виде свертки\n",
    "\n",
    "На прошлом лекции, мы рассмотрели операцию свёртки и отметили, что свёртка — это очень полезная и распространённая операция, лежащая в основе различных фильтров. \n",
    "\n",
    "Одна из важнейших свёрток $-$ это вычисление производных. \n",
    "В математике и физике производные играют очень важную роль, то же самое можно сказать и про компьютерное зрение.\n",
    "Но что же это за производная от изображения? Как мы помним, изображения, с которыми мы работаем, состоят из пикселей, которые, для картинки в градациях серого, задают значение яркости.\n",
    "Т.е. изображение $-$ это просто двумерная матрица чисел. Теперь вспомним, что же такое производная.\n",
    "\n",
    "**Производная (функции в точке)** $-$ это скорость изменения функции (в данной точке). Определяется как предел отношения приращения функции к приращению ее аргумента при стремлении приращения аргумента к нулю.\n",
    "\n",
    "\n",
    "Получается, что, в нашем случае, производная — это отношение значения приращения пикселя по y к значению приращению пикселя по x: $dI = \\frac{dy}{dx}$\n",
    "\n",
    "Работая с изображением I, мы работает с функцией двух переменных $I(x,y)$, т.е. со скалярным полем. Поэтому, более правильно говорить не о производной, а о градиенте изображения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиент\n",
    "\n",
    "Вспомним такое поняте __градиент__ — вектор, своим направлением указывающий направление наибольшего возрастания некоторой величины ${\\displaystyle \\varphi}$. Это очень важное свойство, которое помогает искать минимумы и максимумы функций нескольких переменных. Осознание данного утверждения потребуется нам дальше на летней смене. \n",
    "\n",
    "Давйте рассмотрим графическую интерпретацию на картинке ниже. Видно, что стрелки идут от синей области к красной, т.е. красная область — пик максимума (вершина), а синяя — минимум (яма).\n",
    "\n",
    "<img src=\"https://d.radikal.ru/d28/1905/e5/9a65c30ea355.jpg\" width=\"500\">\n",
    "\n",
    "Для случая трёхмерного пространства градиентом скалярной функции ${\\displaystyle \\varphi =\\varphi (x,y,z)}$ координат ${\\displaystyle x}, {\\displaystyle y}, {\\displaystyle z}$ называется векторная функция с компонентами ${\\displaystyle \\frac  {\\partial \\varphi }{\\partial x}},{\\displaystyle \\frac  {\\partial \\varphi }{\\partial y}},{\\displaystyle \\frac  {\\partial \\varphi }{\\partial z}}$ и обозначается ${\\displaystyle \\nabla \\varphi =\\left({\\frac {\\partial \\varphi }{\\partial x}},\\;{\\frac {\\partial \\varphi }{\\partial y}},\\;{\\frac {\\partial \\varphi }{\\partial z}}\\right).}$\n",
    "\n",
    "Если ${\\displaystyle \\varphi }$  — функция ${\\displaystyle n}$ переменных ${\\displaystyle x_{1},\\;\\ldots ,\\;x_{n}}$, то её градиентом называется ${\\displaystyle n}$-мерный вектор ${\\displaystyle \\left({\\frac {\\partial \\varphi }{\\partial x_{1}}},\\;\\ldots ,\\;{\\frac {\\partial \\varphi }{\\partial x_{n}}}\\right),} $ компоненты которого равны частным производным ${\\displaystyle \\varphi }$ по всем её аргументам.\n",
    "\n",
    "Если каждой точке M области многомерного пространства поставлено в соответствие некоторое (обычно $-$ действительное) число $u$, то говорят, что в этой области задано скалярное поле.\n",
    "\n",
    "\n",
    "Итак, градиент для каждой точки изображения (функция яркости) — двумерный вектор, компонентами которого являются производные яркости изображения по горизонтали и вертикали $-$ $grad[I(x,y)] = (\\frac{dI}{dx}, \\frac{dI}{dy})$\n",
    "\n",
    "В каждой точке изображения градиентный вектор ориентирован в направлении наибольшего увеличения яркости, а его длина соответствует величине изменения яркости.\n",
    "\n",
    "вектор (в заданной точке) задаётся двумя значениями: длиной и направлением.\n",
    "\n",
    "* длинной: $\\sqrt{dx^2 + dy^2}$;\n",
    "\n",
    "* направление $-$ угол между вектором и осью $x$: $\\tan^{-1}(\\frac{dy}{dx})$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оператор Собеля\n",
    "\n",
    "**Оператор Собеля** $-$ это дискретный дифференциальный оператор, вычисляющий приближение градиента яркости изображения.\n",
    "Оператор вычисляет градиент яркости изображения в каждой точке. Так находится направление наибольшего увеличения яркости и величина её изменения в этом направлении. Результат показывает, насколько «резко» или «плавно» меняется яркость изображения в каждой точке, а значит, вероятность нахождения точки на грани, а также ориентацию границы.\n",
    "\n",
    "Т.о. результатом работы оператора Собеля в точке области постоянной яркости будет нулевой вектор, а в точке, лежащей на границе областей различной яркости — вектор, пересекающий границу в направлении увеличения яркости.\n",
    "\n",
    "Наиболее часто оператор Собеля применяется в алгоритмах выделения границ. \n",
    "\n",
    "Оператор Собеля основан на свёртке изображения небольшими целочисленными фильтрами в вертикальном и горизонтальном направлениях, поэтому его относительно легко вычислять. Оператор использует ядра 3x3, с которыми свёртывают исходное изображение для вычисления приближенных значений производных по горизонтали и по вертикали.\n",
    "\n",
    "\n",
    "### Формализация\n",
    "Пусть ${\\displaystyle \\mathbf {A} }$ $-$ это исходное изображение, а ${\\displaystyle \\mathbf {G} _{x}}$ и ${\\displaystyle \\mathbf {G} _{y}}$ $-$ два изображения, на которых каждая точка содержит приближённые производные по ${\\displaystyle x}$ и по ${\\displaystyle y}$. Они вычисляются следующим образом:\n",
    "\n",
    "${\\displaystyle \\mathbf {G} _{y}={\\begin{bmatrix}-1&-2&-1\\\\0&0&0\\\\+1&+2&+1\\\\\\end{bmatrix}}*\\mathbf {A} \\quad {\\mbox{and}}\\quad \\mathbf {G} _{x}={\\begin{bmatrix}-1&0&+1\\\\-2&0&+2\\\\-1&0&+1\\end{bmatrix}}*\\mathbf {A} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Контур и как его найти"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Контурный анализ** $-$ это один из важных и очень полезных методов описания, хранения, распознавания, сравнения и поиска графических образов/объектов. \n",
    "\n",
    "**Контур** $-$ это внешние очертания (обвод) предмета/объекта.\n",
    "\n",
    "При проведении контурного анализа:\n",
    "* полагается, что контур содержит достаточную информацию о форме объекта;\n",
    "* внутренние точки объекта во внимание не принимаются. \n",
    "\n",
    "Вышеприведённые положения, разумеется, накладывают существенные ограничения на область применения контурного анализа, которые, в основном, связаны с проблемами выделения контура на изображениях:\n",
    "* из-за одинаковой яркости с фоном объект может не иметь чёткой границы, или может быть зашумлён помехами, что приводит к невозможности выделения контура;\n",
    "* перекрытие объектов или их группировка приводит к тому, что контур выделяется неправильно и не соответствует границе объекта.\n",
    "\n",
    "Однако, переход к рассмотрению только контуров объектов позволяет уйти от пространства изображения – к пространству контуров, что существенно снижает сложность алгоритмов и вычислений. \n",
    "\n",
    "Т.о., контурный анализ имеет довольно слабую устойчивость к помехам, и любое пересечение или лишь частичная видимость объекта приводит либо к невозможности детектирования, либо к ложным срабатываниям, но простота и быстродействие контурного анализа, позволяют вполне успешно применять данный подход (при чётко выраженном объекте на контрастном фоне и отсутствии помех).\n",
    "\n",
    "Итак, мы определились, что контур — это некая граница объекта, которая отделяет его от фона (других объектов). \n",
    "\n",
    "Во всех случаях мы получаем бинарное изображение, которое явным образом задаёт нам границы объекта. Вот эта совокупность пикселей, составляющих границу объекта и есть контур объекта.\n",
    "\n",
    "Чтобы оперировать полученным контуром, его необходимо как-то представить (закодировать). \n",
    "Например, указывать вершины отрезков, составляющих контур.\n",
    "Другой известный способ кодирования контура $-$ это **цепной код Фримена**. Этот метод будет рассмотрен чуть позже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оператор Лапласа\n",
    "Он вычисляет лапласиан изображения, заданного соотношением,\n",
    "${\\Delta src = \\frac{\\partial ^2{src}}{\\partial x^2} + \\frac{\\partial ^2{src}}{\\partial y^2}}$\n",
    "где каждая производная находится с использованием производных Собеля. Если ksize = $3$, то для фильтрации используется следующее ядро:\n",
    "\n",
    "$$\n",
    "{K = \\begin{pmatrix}\n",
    "0 & \\ 1 & \\ 0 \\\\ \n",
    "1 & \\ -4 & \\ 1 \\\\ \n",
    "0 & \\ 1 & \\ 0 \n",
    "\\end{pmatrix}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:20:58.242702Z",
     "start_time": "2020-07-18T05:20:57.943344Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:21:19.346571Z",
     "start_time": "2020-07-18T05:21:18.841276Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/lk.jpg')\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "## выделяем границы\n",
    "gray_img = cv2.GaussianBlur(gray_img, ksize=(7, 7), sigmaX=1, sigmaY=1)\n",
    "laplac = cv2.Laplacian(gray_img, cv2.THRESH_BINARY, scale=0.55, ksize=5)\n",
    "\n",
    "fig, m_axs = plt.subplots(1,2, figsize=(14,12))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('gray')\n",
    "ax1.imshow(gray_img, cmap='gray')\n",
    "ax2.set_title('laplaccian')\n",
    "ax2.imshow(laplac, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детектор границ Кенни (Canny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теория\n",
    "**Canny Edge Detection** $-$ популярный алгоритм обнаружения краев. Это многоступенчатый алгоритм, и мы пройдем через все этапы.\n",
    "\n",
    "1. **Шумоподавление**\n",
    "\n",
    "Поскольку обнаружение краев подвержено воздействию шума на изображении, первым шагом является удаление шума на изображении с помощью фильтра Гаусса $5\\times5$. Мы уже видели это в предыдущих главах.\n",
    "\n",
    "2. **Поиск градиента интенсивности изображения**\n",
    "\n",
    "Затем сглаженное изображение фильтруется ядром Собеля (рассмотрен выше) в горизонтальном и вертикальном направлении, чтобы получить первую производную в горизонтальном направлении ($G_x$) и вертикальном направлении ($G_y$). Из этих двух изображений мы можем найти градиент края и направление для каждого пикселя следующим образом:\n",
    "\n",
    "$${Edge(G) = \\sqrt{ G_x^2 + G_y^2}}$$\n",
    "$${Angle(\\theta) = \\tan^{-1}(\\frac{G_x}{G_y})}$$\n",
    "\n",
    "Направление градиента всегда перпендикулярно краям. Он округлен до одного из четырех углов, представляющих вертикальное, горизонтальное и два диагональных направления.\n",
    "\n",
    "3. **Немаксимальное подавление**\n",
    "\n",
    "После получения величины и направления градиента выполняется полное сканирование изображения для удаления любых нежелательных пикселей, которые могут не составлять края. Для этого в каждом пикселе пиксель проверяется, является ли он локальным максимумом в его окрестности в направлении градиента. Проверьте изображение ниже:\n",
    "\n",
    "<img src=\"https://i.ibb.co/XZZVNmK/nms.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/> \n",
    "\n",
    "Точка $А$ находится на краю (в вертикальном направлении). Направление градиента нормальное к краю. Точки $B$ и $C$ находятся в градиентных направлениях. Таким образом, точка $A$ проверяется с помощью точек $B$ и $C$, чтобы увидеть, образует ли она локальный максимум. Если это так, он рассматривается для следующего этапа, в противном случае он подавляется (обнуляется).\n",
    "\n",
    "Короче говоря, в результате вы получите бинарное изображение с «тонкими краями».\n",
    "\n",
    "4. **Подбор порогового значения**\n",
    "\n",
    "Эта стадия решает, какие ребра действительно являются ребрами, а какие нет. Для этого нам понадобятся два пороговых значения, **minVal** и **maxVal**. Любые ребра с градиентом интенсивности, превышающим **maxVal**, обязательно будут ребрами, а ребра ниже **minVal** не будут ребрами, поэтому отбрасываются. Те, кто лежит между этими двумя порогами, классифицируются как ребра или не ребра в зависимости от их связности. Если они связаны с точными пикселями, они считаются частью ребер. В противном случае они также отбрасываются. Смотрите изображение ниже:\n",
    "\n",
    "<img src=\"https://i.ibb.co/cryRqvD/hysteresis.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/> \n",
    "\n",
    "Край $A$ выше **maxVal**, так что считается «верным краем». Хотя ребро $C$ меньше **maxVal**, оно связано с ребром $A$, так что это также считается допустимым ребром, и мы получаем эту полную кривую. Но ребро $B$, хотя оно выше **minVal** и находится в той же области, что и ребро $C$, не связано с каким-либо «верным краем», поэтому отбрасывается. Поэтому очень важно, чтобы мы выбрали соответственно **minVal** и **maxVal**, чтобы получить правильный результат.\n",
    "\n",
    "На этом этапе также удаляются небольшие пиксельные шумы в предположении, что края являются длинными линиями.\n",
    "\n",
    "Итак, что мы в итоге получаем, это сильные края изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детектор границ Кенни в OpenCV\n",
    "OpenCV помещает все вышеперечисленное в одну функцию ```cv2.Canny(image,threshold1,threshold2,apertureSize,L2gradient)```. \n",
    "\n",
    "* **image** $-$ это наше входное изображение\n",
    "* **threshold1** $-$ minVal для процедуры гистерезиса\n",
    "* **threshold2** $-$ maxVal для процедуры гистерезиса\n",
    "* **apertureSize** $-$ размер ядра Собеля, используемый для поиска градиентов изображения, по умолчанию равен $3$\n",
    "* **L2gradient** $-$ флаг, определяет уравнение для определения величины градиента. Если это True, он использует упомянутое выше уравнение, которое является более точным, в противном случае он использует эту функцию: Edge_Gradient($G$) = |$G_x$| + |$G_y$|. По умолчанию это False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:21:34.987086Z",
     "start_time": "2020-07-18T05:21:34.983101Z"
    }
   },
   "outputs": [],
   "source": [
    "def auto_canny(image, sigma=0.33):\n",
    "    # compute the median of the single channel pixel intensities\n",
    "    v = np.median(image)\n",
    "\n",
    "    # apply automatic Canny edge detection using the computed median\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    edged = cv2.Canny(image, lower, upper)\n",
    "\n",
    "    # return the edged image\n",
    "    return edged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:21:49.709620Z",
     "start_time": "2020-07-18T05:21:49.276859Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/lk.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "edges = auto_canny(blurred)\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Original Image')\n",
    "ax1.imshow(img, cmap='gray')\n",
    "ax2.set_title('Edge Image')\n",
    "ax2.imshow(edges, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:22:25.813776Z",
     "start_time": "2020-07-18T05:22:25.563377Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/RGB_cube.png')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (3, 3), 1)\n",
    "\n",
    "edges = auto_canny(blurred)\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Original Image')\n",
    "ax1.imshow(img, cmap='gray')\n",
    "ax2.set_title('Edge Image')\n",
    "ax2.imshow(edges, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оператор Laplacian of Gaussian (LoG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы узнали, как использовать оператор Собеля. Это было основано на том факте, что в краевой области интенсивность пикселей показывает «скачок» или большое изменение интенсивности. Получив первую производную интенсивности, мы заметили, что ребро характеризуется максимумом, как это видно на рисунке:\n",
    "\n",
    "<img src=\"img/Laplace_Operator_Tutorial_Theory_Previous.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/> \n",
    "\n",
    "\n",
    "И ... что произойдет, если мы возьмем вторую производную?\n",
    "\n",
    "<img src=\"img/Laplace_Operator_Tutorial_Theory_ddIntensity.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "\n",
    "\n",
    "Вы можете заметить, что вторая производная равна нулю. Таким образом, мы также можем использовать этот критерий, чтобы попытаться обнаружить края на изображении. Однако обратите внимание, что нули будут появляться не только по краям (они могут фактически появляться в других бессмысленных местах); это может быть решено путем применения фильтрации там, где это необходимо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian Operator\n",
    "\n",
    "Из приведенного выше объяснения мы делаем вывод, что вторая производная может использоваться для поиска ребер. Поскольку изображения $2D$, нам нужно взять производную в обоих измерениях. Здесь удобен оператор Лапласа, который определяется:\n",
    "\n",
    "${Laplace(f) = \\Delta {f} = \\frac{\\partial ^2{f}}{\\partial x^2} + \\frac{\\partial ^2{f}}{\\partial y^2}}$\n",
    "\n",
    "\n",
    "Оператор Лапласа реализован в OpenCV функцией ```cv2.Laplacian()```. Фактически, поскольку лапласиан использует градиент изображений, он вызывает внутренний оператор Собеля для выполнения своих вычислений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаги для фильтра LoG:\n",
    "\n",
    "* Применить LoG на изображение, Это можно сделать двумя способами:\n",
    "     * Сначала примените гауссовский, а затем лапласианский или\n",
    "     * Конвертировать изображение с помощью ядра LoG напрямую\n",
    "* Найти нулевые пересечения в изображении\n",
    "* Отобрать по порогу пересечения нуля, чтобы извлечь только \"сильные\" края."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Немного математики\n",
    "\n",
    "Для начала посмотрим на LoG. Уже поняли, что такое оператор лапласа или (лапласиан), на прошлом занятии уже рассмотрели гауссиану $G_{\\sigma}(x,y)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{x^2+y^2}{2\\sigma^2}\\right)$. \n",
    "\n",
    "**Вопрос:** чему равно $\\Delta{G_{sigma}}= \\ ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практика\n",
    "\n",
    "1. Чему равно $\\Delta{G_{sigma}}= \\ ?$ (аналитический вид)\n",
    "\n",
    "2. Постройте графики $G_{sigma}(x,y)$, $\\frac{\\partial {G(x, y)}}{\\partial {x}}$, $\\Delta{G_{sigma}(x, y)}$ при $x \\in [-7, 7]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перейдем к реализации фильтра "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:25:50.672193Z",
     "start_time": "2020-07-18T05:25:50.661606Z"
    }
   },
   "outputs": [],
   "source": [
    "def LoG_filter(image, kernel_size=5, sigma=0):\n",
    "    # применить гауссов фильтр\n",
    "    blur = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n",
    "\n",
    "    # применить лапласиан\n",
    "    laplacian = cv2.Laplacian(blur, cv2.CV_64F, kernel_size, delta=sigma)\n",
    "\n",
    "    # глобальная нормировка на максимум\n",
    "    laplacian1 = laplacian / laplacian.max()\n",
    "    \n",
    "    z_c_image = np.zeros(image.shape)\n",
    "    \n",
    "    # для каждого пикселя считаем количество положительных и отрицательных значений\n",
    "    neighbour_size = 1\n",
    "    for i in range(neighbour_size, image.shape[0] - neighbour_size):\n",
    "        for j in range(neighbour_size, image.shape[1] - neighbour_size):\n",
    "            negative_count = 0\n",
    "            positive_count = 0\n",
    "            # соседи\n",
    "            neighbour = [laplacian1[i+1, j-1],\n",
    "                         laplacian1[i+1, j],\n",
    "                         laplacian1[i+1, j+1],\n",
    "                         laplacian1[i, j-1],\n",
    "                         laplacian1[i, j+1],\n",
    "                         laplacian1[i-1, j-1],\n",
    "                         laplacian1[i-1, j],\n",
    "                         laplacian1[i-1, j+1]]\n",
    "            d = np.max(neighbour)\n",
    "            e = np.min(neighbour)\n",
    "            for h in neighbour:\n",
    "                if h > 0:\n",
    "                    positive_count += 1\n",
    "                elif h < 0:\n",
    "                    negative_count += 1\n",
    "\n",
    "\n",
    "            # Если и отрицательные, и положительные значения существуют в \n",
    "            # районе пикселя, тогда этот пиксель - это потенциальный нулевой переход\n",
    "            \n",
    "            z_c = ((negative_count > 0) and (positive_count > 0))\n",
    "            \n",
    "            # Измениnm значение пикселя на максимальное по соседству разницу с пикселем\n",
    "            if z_c:\n",
    "                if image[i,j] > 0:\n",
    "                    z_c_image[i, j] = image[i,j] + np.abs(e)\n",
    "                elif image[i,j] < 0:\n",
    "                    z_c_image[i, j] = np.abs(image[i,j]) + d\n",
    "                \n",
    "    # нормилизация на 255\n",
    "    z_c_norm = z_c_image / z_c_image.max() * 255\n",
    "    z_c_image = np.uint8(z_c_norm)\n",
    "\n",
    "    return z_c_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:27:11.224950Z",
     "start_time": "2020-07-18T05:27:05.693571Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/RGB_cube.png')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "edges = LoG_filter(gray)\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Original Image')\n",
    "ax1.imshow(img, cmap='gray')\n",
    "ax2.set_title('Edge Image')\n",
    "ax2.imshow(edges, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оператор Difference of Gaussians (DoG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учитывая $m$ - канал, $n$ - мерное изображение\n",
    "\n",
    "$I:\\{\\mathbb{X}\\subseteq\\mathbb{R}^n\\}\\rightarrow\\{\\mathbb{Y}\\subseteq\\mathbb{R}^m\\}$\n",
    "\n",
    "Разность гауссианов (DoG) изображения $I$ - это функция\n",
    "\n",
    "$\\Gamma_{\\sigma_1,\\sigma_2}:\\{\\mathbb{X}\\subseteq\\mathbb{R}^n\\}\\rightarrow\\{\\mathbb{Z}\\subseteq\\mathbb{R}\\}$\n",
    "\n",
    "полученный путем вычитания изображения $I$, свернутого с дисперсией Гаусса $\\sigma^2_2$, из изображения $I$, свернутого с гауссианом более узкой дисперсии $\\sigma^2_1 $, с $\\sigma_2 > \\sigma_1$. В одном измерении $\\Gamma$ определяется как:\n",
    "\n",
    "$\\Gamma_{\\sigma_1,\\sigma_2}(x)=I*\\frac{1}{\\sigma_1\\sqrt{2\\pi}} \\, e^{-(x^2)/(2\\sigma^2_1)}-I*\\frac{1}{\\sigma_2\\sqrt{2\\pi}} \\, e^{-(x^2)/(2\\sigma_2^2)}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практика\n",
    "\n",
    "1. Постройте графики $G_{\\sigma_1}$ и $G_{\\sigma_2}$\n",
    "2. Примените DoG filter по аналогии с LoG\n",
    "3. Сравните результаты применения LoG и DoG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Контуры\n",
    "\n",
    "Полученные границы достаточно просто преобразуются в контуры. Для алгоритма Кэнни это происходит автоматически, для остальных алгоритмов требуется дополнительная бинаризация. Получить контур для бинарного алгоритма можно например алгоритмом [жука](http://wiki.technicalvision.ru/index.php/%D0%92%D1%8B%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B8_%D0%BE%D0%BF%D0%B8%D1%81%D0%B0%D0%BD%D0%B8%D0%B5_%D0%BA%D0%BE%D0%BD%D1%82%D1%83%D1%80%D0%BE%D0%B2).\n",
    "\n",
    "В OpenCV поиск контуров похож на поиск белого объекта на черном фоне. Помните, что объект, который нужно найти, должен быть белым, а фон должен быть черным.\n",
    "Давайте посмотрим, как найти контуры двоичного изображения с помощью метода **Фридмана**:\n",
    "\n",
    "**Цепной код Фримена (Фридмана) (Freeman Chain Code)**\n",
    "\n",
    "Цепные коды применяются для представления границы в виде последовательности отрезков прямых линий определённой длины и направления. В основе этого представления лежит 4- или 8- связная решётка. Длина каждого отрезка определяется разрешением решётки, а направления задаются выбранным кодом.\n",
    "(для представления всех направлений в 4-связной решётке достаточно 2-х бит, а для 8-связной решётки цепного кода требуется 3 бита)\n",
    "\n",
    "<img src=\"https://i.ibb.co/6tyGLKS/freeman_chain_code.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "\n",
    "Если честно, то у меня ни разу ни получилось применить контурный анализ в реальных задачах. Уж слишком идеальные условия требуются. То граница не найдётся, то шумов слишком много. Но, если нужно что-то распознавать в идеальных условиях $-$ то контурный анализ замечательный вариант. Очень быстро работает, красивая математика и понятная логика."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В OpenCV для поиска контуров имеется функцией findContours, которая имеет вид:\n",
    "\n",
    "```findContours(img, hierarchy, mode, method, offset)```\n",
    "\n",
    "\n",
    "1. img — должным образом подготовленная для анализа картинка. Это должно быть 8-битное изображение. Поиск контуров использует для работы монохромное изображение, так что все пиксели картинки с ненулевым цветом будут интерпретироваться как 1, а все нулевые останутся нулями.\n",
    "\n",
    "\n",
    "2. mode — один из четырех режимов группировки найденных контуров:\n",
    "    * CV_RETR_LIST — выдаёт все контуры без группировки;\n",
    "    * CV_RETR_EXTERNAL — выдаёт только крайние внешние контуры. Например, если в кадре будет пончик, то функция вернет его внешнюю границу без дырки.\n",
    "    * CV_RETR_CCOMP — группирует контуры в двухуровневую иерархию. На верхнем уровне — внешние контуры объекта. На втором уровне — контуры отверстий, если таковые имеются. Все остальные контуры попадают на верхний уровень.\n",
    "    * CV_RETR_TREE — группирует контуры в многоуровневую иерархию.\n",
    "\n",
    "\n",
    "3. method — один из трёх методов упаковки контуров:\n",
    "    * CV_CHAIN_APPROX_NONE — упаковка отсутствует и все контуры хранятся в виде отрезков, состоящих из двух пикселей.\n",
    "    * CV_CHAIN_APPROX_SIMPLE — склеивает все горизонтальные, вертикальные и диагональные контуры.\n",
    "    * CV_CHAIN_APPROX_TC89_L1,CV_CHAIN_APPROX_TC89_KCOS — применяет к контурам метод упаковки (аппроксимации) Teh-Chin.\n",
    "\n",
    "\n",
    "4. hierarchy — список всех найденных контуров, представленных в виде векторов; иерархия — информация о топологии контуров. Каждый элемент иерархии представляет собой сборку из четырех индексов, которая соответствует контуру[i]:\n",
    "    * иерархия[i][0] — индекс следующего контура на текущем слое;\n",
    "    * иерархия[i][1] — индекс предыдущего контура на текущем слое:\n",
    "    * иерархия[i][2] — индекс первого контура на вложенном слое;\n",
    "    * иерархия[i][3] — индекс родительского контура.\n",
    "\n",
    "\n",
    "5. offset — величина смещения точек контура. Это полезно, если контуры извлекаются из ROI, а затем они должны анализироваться во всем контексте изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:29:28.694724Z",
     "start_time": "2020-07-18T05:29:28.551441Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/RGB_cube.png')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "laplac = cv2.Laplacian(gray_img, cv2.THRESH_BINARY, scale=0.15, ksize=5)\n",
    "laplac = cv2.medianBlur(laplac, 3)\n",
    "contours, hierarchy = cv2.findContours(laplac, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "plt.imshow(laplac, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как нарисовать контуры?\n",
    "\n",
    "Для рисования контуров используется функция:\n",
    "\n",
    "```drawContours(image, contours, contourIdx, color, thickness, lineType, hierarchy, maxLevel, offset)```\n",
    "\n",
    "1. image — кадр, поверх которого мы будем отрисовывать контуры; \n",
    "\n",
    "\n",
    "2. contours — те самые контуры, найденные функцией findContours; \n",
    "\n",
    "\n",
    "3. contourIdx — индекс контура, который следует отобразить. \n",
    "    * -1 — если нужно отобразить все контуры; \n",
    "\n",
    "\n",
    "4. color — цвет контура; \n",
    "\n",
    "\n",
    "5. thickness — толщина линии контура; \n",
    "\n",
    "\n",
    "6. lineType — тип соединения точек вектора; \n",
    "\n",
    "\n",
    "7. hierarchy — информация об иерархии контуров; \n",
    "\n",
    "\n",
    "8. maxLevel — индекс слоя, который следует отображать. \n",
    "    * Если параметр равен 0, то будет отображен только выбранный контур. Если параметр равен 1, то отобразится выбранный контур и все его дочерние контуры. Если параметр равен 2, то отобразится выбранный контур, все его дочерние и дочерние дочерних! И так далее. \n",
    "    \n",
    "\n",
    "9. offset — величина смещения точек контура.\n",
    "\n",
    "Его также можно использовать для рисования любой фигуры, если у вас есть граничные точки. Его первый аргумент является исходным изображением, второй аргумент - это контуры, которые должны быть переданы в виде списка Python, третий аргумент - это индекс контуров (полезно при рисовании отдельного контура. Чтобы нарисовать все контуры, передайте -1), а остальные аргументы - это цвет, толщина и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:29:40.975255Z",
     "start_time": "2020-07-18T05:29:40.820413Z"
    }
   },
   "outputs": [],
   "source": [
    "## нарисуем все найденные контуры\n",
    "img1 = gray_img.copy()\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "cv2.drawContours(img1, contours, -1, (255, 0, 0), 3)\n",
    "\n",
    "plt.imshow(img1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:29:47.809334Z",
     "start_time": "2020-07-18T05:29:47.614801Z"
    }
   },
   "outputs": [],
   "source": [
    "## нарисуем один выбранный контур\n",
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "cv2.drawContours(img2, contours, 53, (255, 0, 0), 10)\n",
    "\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ключевые признаки контура\n",
    "\n",
    "Пройдемся по основным методам работы с характеристиками конутра, которые доступны в OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Моменты\n",
    "Моменты изображения помогают вам рассчитать некоторые функции, такие как центр масс объекта, площадь объекта и т. д.\n",
    "\n",
    "Функция **cv2.moments()** предоставляет словарь всех вычисленных значений моментов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Центральные моменты__\n",
    "\n",
    "$\\mu _{{pq}}=\\sum _{{x}}\\sum _{{y}}(x-{\\bar  {x}})^{p}(y-{\\bar  {y}})^{q}f(x,y)$\n",
    "\n",
    "__Масштабные инварианты__\n",
    "\n",
    "Инварианты $η_{ij}$ относительно сдвига и масштаба могут быть построены из центральных моментов путем деления на правильно масштабированный нулевой центральный момент:\n",
    "\n",
    "${\\displaystyle \\eta _{ij}={\\frac {\\mu _{ij}}{\\mu _{00}^{\\left(1+{\\frac {i+j}{2}}\\right)}}}\\}\\$, \n",
    "где i + j ≥ 2. Обратите внимание, что трансляционная инвариантность непосредственно следует только за счет использования центральных моментов.\n",
    "\n",
    "__Вращающиеся инварианты__\n",
    "\n",
    "Как показано в работе Ху, могут быть построены инварианты относительно перемещения, масштаба и вращения:\n",
    "\n",
    "${\\displaystyle I_{1}=\\eta _{20}+\\eta _{02}} I_{1}=\\eta _{{20}}+\\eta _{{02}}$\n",
    "\n",
    "${\\displaystyle I_{2}=(\\eta _{20}-\\eta _{02})^{2}+4\\eta _{11}^{2}} I_{2}=(\\eta _{{20}}-\\eta _{{02}})^{2}+4\\eta _{{11}}^{2}$\n",
    "\n",
    "${\\displaystyle I_{3}=(\\eta _{30}-3\\eta _{12})^{2}+(3\\eta _{21}-\\eta _{03})^{2}} I_{3}=(\\eta _{{30}}-3\\eta _{{12}})^{2}+(3\\eta _{{21}}-\\eta _{{03}})^{2}$\n",
    "\n",
    "${\\displaystyle I_{4}=(\\eta _{30}+\\eta _{12})^{2}+(\\eta _{21}+\\eta _{03})^{2}} I_{4}=(\\eta _{{30}}+\\eta _{{12}})^{2}+(\\eta _{{21}}+\\eta _{{03}})^{2}$\n",
    "\n",
    "${\\displaystyle I_{5}=(\\eta _{30}-3\\eta _{12})(\\eta _{30}+\\eta _{12})[(\\eta _{30}+\\eta _{12})^{2}-3(\\eta _{21}+\\eta _{03})^{2}]+(3\\eta _{21}-\\eta _{03})(\\eta _{21}+\\eta _{03})[3(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{21}+\\eta _{03})^{2}]}$\n",
    "\n",
    "${\\displaystyle I_{6}=(\\eta _{20}-\\eta _{02})[(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{21}+\\eta _{03})^{2}]+4\\eta _{11}(\\eta _{30}+\\eta _{12})(\\eta _{21}+\\eta _{03})}$\n",
    "\n",
    "${\\displaystyle I_{7}=(3\\eta _{21}-\\eta _{03})(\\eta _{30}+\\eta _{12})[(\\eta _{30}+\\eta _{12})^{2}-3(\\eta _{21}+\\eta _{03})^{2}]-(\\eta _{30}-3\\eta _{12})(\\eta _{21}+\\eta _{03})[3(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{21}+\\eta _{03})^{2}].}$\n",
    "\n",
    "${\\displaystyle I_{8}=\\eta _{11}[(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{03}+\\eta _{21})^{2}]-(\\eta _{20}-\\eta _{02})(\\eta _{30}+\\eta _{12})(\\eta _{03}+\\eta _{21})}$\n",
    "\n",
    "${\\displaystyle I_{8}=\\eta _{11}[(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{03}+\\eta _{21})^{2}]-(\\eta _{20}-\\eta _{02})(\\eta _{30}+\\eta _{12})(\\eta _{03}+\\eta _{21})}$\n",
    "\n",
    "Они хорошо известны как инварианты моментов Ху.\n",
    "\n",
    "Первый, I1, аналогичен моменту инерции вокруг центроида изображения, где интенсивности пикселей аналогичны физической плотности. Последний, I7, является косоинвариантным, что позволяет ему отличать зеркальные изображения от других идентичных изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:30:18.701756Z",
     "start_time": "2020-07-18T05:30:18.681143Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/RGB_cube.png')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "laplac = cv2.Laplacian(gray_img, cv2.THRESH_BINARY, scale=1, ksize=5)\n",
    "laplac = cv2.medianBlur(laplac, 3)\n",
    "contours, hierarchy = cv2.findContours(laplac, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "cnt = contours[100]\n",
    "M = cv2.moments(cnt)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:30:20.008021Z",
     "start_time": "2020-07-18T05:30:20.004059Z"
    }
   },
   "outputs": [],
   "source": [
    "## Коэффициент асимметрии\n",
    "import math\n",
    "sigma_x = math.sqrt(M['m20']/M['m00'])\n",
    "sigma_y = math.sqrt(M['m02']/M['m00'])\n",
    "\n",
    "k_x = M['m30']/sigma_x**3\n",
    "k_y = M['m03']/sigma_y**3\n",
    "\n",
    "print(k_x, k_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из этих моментов вы можете извлечь полезные данные, такие как площадь, центроид и т.д. Центроид определяется отношениями, ${C_x = \\frac{M_{10}}{M_{00}}}$ and ${C_y = \\frac{M_{01}}{M_{00}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:30:36.516852Z",
     "start_time": "2020-07-18T05:30:36.369084Z"
    }
   },
   "outputs": [],
   "source": [
    "Cx = int(M['m10'] /M ['m00'])\n",
    "Cy = int(M['m01'] / M['m00'])\n",
    "print('Cx =', Cx, 'Cy =', Cy)\n",
    "\n",
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "cnt = contours[100]\n",
    "cv2.drawContours(img2, [cnt], 0, (255, 0, 0), 3)\n",
    "plt.scatter(Cx, Cy, color='blue')\n",
    "\n",
    "plt.grid()\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы рассчитать все моменты Ху есть функция ```cv2.HuMoments(moments)````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:30:39.998403Z",
     "start_time": "2020-07-18T05:30:39.940238Z"
    }
   },
   "outputs": [],
   "source": [
    "hu = cv2.HuMoments(M)\n",
    "\n",
    "print(hu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Пример сохранения моментов__\n",
    "\n",
    "<img src=\"https://i.ibb.co/1nL7q4w/HuMoments.png\" alt=\"Drawing\" style=\"width: 600px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Расстояние между двумя фигурами с помощью matchShapes\n",
    "\n",
    "В этом разделе мы узнаем, как использовать моменты Ху, чтобы найти расстояние между двумя фигурами. Если расстояние маленькое, формы близки по внешнему виду, а если расстояние большое, то фигуры находятся дальше друг от друга по внешнему виду.\n",
    "\n",
    "OpenCV предоставляет простую в использовании служебную функцию matchShapes, которая берет два изображения (или контура) и находит расстояние между ними с помощью Hu Moments. Таким образом, вам не нужно явно вычислять моменты Ху. Просто оцифруйте изображения и используйте matchShapes.\n",
    "\n",
    "Использование показано ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:30:46.275808Z",
     "start_time": "2020-07-18T05:30:45.827741Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/lk.jpg')\n",
    "## для отрисовки в pyplot\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "rows, cols = img.shape\n",
    "\n",
    "M1 = cv2.getRotationMatrix2D((cols/2, rows/2), 25, scale=1.0)\n",
    "M2 = cv2.getRotationMatrix2D((300, 700), -15, scale=0.75)\n",
    "M3 = cv2.getRotationMatrix2D((300, 100), 45, scale=2.0)\n",
    "\n",
    "## визуализация\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(20,8))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "dst1 = cv2.warpAffine(img.copy(), M1, (cols, rows))\n",
    "ax1.imshow(dst1,  cmap='gray')\n",
    "ax1.grid()\n",
    "ax1.set_title('M1 преоразование', fontsize=15)\n",
    "dst2 = cv2.warpAffine(img.copy(), M2, (cols, rows))\n",
    "ax2.imshow(dst2,  cmap='gray')\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:34:00.806242Z",
     "start_time": "2020-07-18T05:34:00.796693Z"
    }
   },
   "outputs": [],
   "source": [
    "d1 = cv2.matchShapes(dst1, dst2, cv2.CONTOURS_MATCH_I1,0)\n",
    "d2 = cv2.matchShapes(dst1, dst2, cv2.CONTOURS_MATCH_I2,0)\n",
    "d3 = cv2.matchShapes(dst1, dst2, cv2.CONTOURS_MATCH_I3,0)\n",
    "\n",
    "print(f'Расстояние по I1: {d1:.5f}\\n'\n",
    "      f'Расстояние по I2: {d2:.5f}\\n'\n",
    "      f'Расстояние по I3: {d3:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что есть три вида расстояний, которые можно использовать с помощью третьего параметра (CONTOURS_MATCH_I1, CONTOURS_MATCH_I2 или CONTOURS_MATCH_I3).\n",
    "\n",
    "Два изображения (im1 и im2) похожи, если указанные выше расстояния малы. Вы можете использовать любую меру расстояния. Они обычно дают похожие результаты.\n",
    "\n",
    "Давайте посмотрим, как определяются эти три расстояния.\n",
    "\n",
    "Пусть $ D (A, B) $ - расстояние между формами $ A $ и $ B $, а $ H ^ A_i $ и $ H ^ B_i $ - логарифмические преобразования $ i ^ {th} $ Ху Моментов для фигур $ A $ и $ B $. Расстояния, соответствующие трем случаям, определяются как\n",
    "\n",
    "CONTOURS_MATCH_I1\n",
    "   \\begin{align*} D(A, B) = \\sum^{6}_{i=0} \\left | \\frac{1}{H^B_i} - \\frac{1}{H^A_i} \\right |  \\end{align*}\n",
    "\n",
    "CONTOURS_MATCH_I2\n",
    "   \\begin{align*} D(A, B) = \\sum^{6}_{i=0} \\left | H^B_i - H^A_i \\right |  \\end{align*}\n",
    "\n",
    "CONTOURS_MATCH_I3\n",
    "   \\begin{align*} D(A, B) = \\sum^{6}_{i=0} \\frac{\\left | H^A_i - H^B_i \\right |}{\\left | H^A_i \\right |}  \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom distance measure\n",
    "Если вы хотите определить собственную меру расстояния между двумя фигурами, вы можете легко это сделать. Например, вы можете использовать евклидово расстояние между моментами Ху, заданными\n",
    "\n",
    "  \\begin{align*} D(A, B) = \\sqrt { \\sum^{6}_{i=0} \\left ( H^B_i - H^A_i \\right )^2 } \\end{align*}\n",
    "\n",
    "Сначала вы вычисляете трансформированные в журнал моменты Ху, как упомянуто в предыдущем разделе, а затем сами вычисляете расстояние вместо использования matchShapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Контурная зона\n",
    "Площадь контура задается функцией **cv2.contourArea()** или из моментов, **M['m00']**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:34:15.427274Z",
     "start_time": "2020-07-18T05:34:15.369045Z"
    }
   },
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "print(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Периметр контура\n",
    "Это также называется длиной дуги. Это можно узнать с помощью функции **cv2.arcLength()**. Второй аргумент указывает, является ли фигура замкнутым контуром (если передан True) или просто кривой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:34:38.181939Z",
     "start_time": "2020-07-18T05:34:38.177950Z"
    }
   },
   "outputs": [],
   "source": [
    "perimeter = cv2.arcLength(cnt, True)\n",
    "print(round(perimeter, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Контурное приближение\n",
    "Он приближает форму контура к другой форме с меньшим количеством вершин в зависимости от заданной нами точности. Это реализация алгоритма Дугласа-Пекера. Проверьте страницу википедии на алгоритм и демонстрацию.\n",
    "\n",
    "Чтобы понять это, предположим, что вы пытаетесь найти квадрат на изображении, но из-за некоторых проблем на изображении вы получили не идеальный квадрат, а \"плохую форму\" (как показано на первом изображении ниже). Теперь вы можете использовать эту функцию для аппроксимации формы. В этом случае второй аргумент называется эпсилон, который является максимальным расстоянием от контура до приближенного контура. Это параметр точности. Для правильного вывода необходим мудрый выбор эпсилона.\n",
    "\n",
    "<img src=\"https://i.ibb.co/jHvc2HS/approx.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "\n",
    "Выше, на втором изображении, зеленая линия показывает приблизительную кривую для эпсилона = $10$% длины дуги. Третье изображение показывает то же самое для эпсилона = $1$% длины дуги. Третий аргумент указывает, является ли кривая замкнутой или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:35:02.563938Z",
     "start_time": "2020-07-18T05:35:02.372814Z"
    }
   },
   "outputs": [],
   "source": [
    "epsilon = 0.1 * cv2.arcLength(cnt,True)\n",
    "approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "\n",
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "cv2.drawContours(img2, [approx], 0, (255, 0, 0), 3)\n",
    "\n",
    "plt.grid()\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выпуклый контур\n",
    "Выпуклая оболочка будет похожа на контурную аппроксимацию, но это не так (оба могут давать одинаковые результаты в некоторых случаях). Здесь функция **cv2.convexHull()** проверяет кривую на наличие дефектов выпуклости и исправляет ее. Вообще говоря, выпуклые кривые $-$ это кривые, которые всегда выпуклые или, по крайней мере, плоские. И если он выпуклый внутри, это называется дефектами выпуклости. Например, проверьте изображение ниже. Красная линия показывает выпуклый корпус руки. Двусторонние стрелки показывают дефекты выпуклости, которые представляют собой локальные максимальные отклонения корпуса от контуров.\n",
    "<img src=\"https://i.ibb.co/Z2nPDCM/convexitydefects.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "**hull = cv2.convexHull(points, clockwise, returnPoints)**\n",
    "\n",
    "* **points** $-$ точки контура.\n",
    "\n",
    "* **clockwise** $-$ флаг ориентации. Если это правда, выходной выпуклый корпус ориентирован по часовой стрелке. В противном случае он ориентирован против часовой стрелки.\n",
    "\n",
    "* **returnPoints** $-$ по умолчанию True. Затем он возвращает координаты точек корпуса. Если False, он возвращает индексы точек контура, соответствующие точкам корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:35:06.660648Z",
     "start_time": "2020-07-18T05:35:06.525737Z"
    }
   },
   "outputs": [],
   "source": [
    "hull = cv2.convexHull(cnt)\n",
    "hull_id = cv2.convexHull(cnt, returnPoints=False)\n",
    "\n",
    "plt.grid()\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но если вы хотите найти дефекты выпуклости, вам нужно передать returnPoints = False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка выпуклости\n",
    "Есть функция, чтобы проверить, является ли кривая выпуклой или нет, **cv2.isContourConvex()**. Это просто возвращает True или False\n",
    "\n",
    "Подумайте, как это можно сделать без этой функции?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:35:09.139247Z",
     "start_time": "2020-07-18T05:35:09.123980Z"
    }
   },
   "outputs": [],
   "source": [
    "k = cv2.isContourConvex(cnt)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ограничивающий прямоугольник\n",
    "\n",
    "#### Прямой ограничивающий прямоугольник\n",
    "Это прямой прямоугольник, он не учитывает вращение объекта. Таким образом, площадь ограничивающего прямоугольника не будет минимальной. Он находится функцией **cv2.boundingRect()**.\n",
    "\n",
    "Пусть $(x,y)$ $-$ верхняя левая координата прямоугольника, а $(w,h)$ $-$ его ширина и высота."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:36:35.501147Z",
     "start_time": "2020-07-18T05:36:35.351970Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y, w, h = cv2.boundingRect(cnt)\n",
    "cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "x, y, w, h = cv2.boundingRect(cnt)\n",
    "cv2.rectangle(img2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "plt.grid()\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Повернутый прямоугольник\n",
    "Здесь ограничивающий прямоугольник рисуется с минимальной площадью, поэтому он учитывает и вращение. Используемая функция $-$ **cv2.minAreaRect()**. Он возвращает структуру **Box2D**, которая содержит следующие детали $-$ (центр $(x, y)$, (ширина, высота), угол поворота). Но чтобы нарисовать этот прямоугольник, нам нужно $4$ угла прямоугольника. Получается функцией **cv2.boxPoints()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:37:16.995207Z",
     "start_time": "2020-07-18T05:37:16.845589Z"
    }
   },
   "outputs": [],
   "source": [
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "\n",
    "rect = cv2.minAreaRect(cnt)\n",
    "box = cv2.boxPoints(rect)\n",
    "box = np.int0(box)\n",
    "cv2.drawContours(img2, [box], 0, (0, 0, 255), 2)\n",
    "\n",
    "plt.grid()\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подгонка линии\n",
    "Точно так же мы можем подогнать линию к набору точек. Ниже изображение содержит набор белых точек. Мы можем приблизить к нему прямую линию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:38:18.274030Z",
     "start_time": "2020-07-18T05:38:18.120391Z"
    }
   },
   "outputs": [],
   "source": [
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "rows, cols = img.shape[:2]\n",
    "vx, vy, x, y = cv2.fitLine(cnt, cv2.DIST_L2, 0, 0.01, 0.01)\n",
    "lefty = int((-x * vy/ vx) + y)\n",
    "righty = int(((cols - x) * vy / vx) + y)\n",
    "cv2.line(img2, (cols-1, righty), (0, lefty), (0, 255, 0), 2)\n",
    "\n",
    "plt.grid()\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свойства контура"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соотношение сторон\n",
    "Это отношение ширины к высоте ограничивающего прямоугольника объекта.\n",
    "\n",
    "${AspectRatio = \\frac{Width}{Height}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:38:39.549822Z",
     "start_time": "2020-07-18T05:38:39.545835Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y, w, h = cv2.boundingRect(cnt)\n",
    "aspect_ratio = float(w) / h\n",
    "\n",
    "print(aspect_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Степень\n",
    "Степень $-$ это отношение площади контура к площади ограничивающего прямоугольника.\n",
    "\n",
    "${Extent=\\frac{Object\\ Area}{Bounding\\ Rectangle\\ Area}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:38:50.420052Z",
     "start_time": "2020-07-18T05:38:50.415060Z"
    }
   },
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "x, y, w, h = cv2.boundingRect(cnt)\n",
    "rect_area = w * h\n",
    "extent = float(area) / rect_area\n",
    "\n",
    "print(extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solidity\n",
    "\n",
    "Solidity $-$ это отношение площади контура к его площади выпуклой оболочки.\n",
    "\n",
    "$Solidity = \\frac{Contour \\ Area}{Convex \\ Hull \\ Area}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:39:05.682349Z",
     "start_time": "2020-07-18T05:39:05.678359Z"
    }
   },
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "hull = cv2.convexHull(cnt)\n",
    "hull_area = cv2.contourArea(hull)\n",
    "solidity = float(area) / hull_area\n",
    "\n",
    "print(solidity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эквивалентный диаметр\n",
    "\n",
    "Эквивалентный диаметр $-$ это диаметр круга, площадь которого равна площади контура.\n",
    "\n",
    "$Equivalent \\ Diameter = \\sqrt{\\frac{4 \\cdot \\ Contour \\ Area}{\\pi}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:39:14.269824Z",
     "start_time": "2020-07-18T05:39:14.266988Z"
    }
   },
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "equi_diameter = np.sqrt(4 * area / np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ориентация\n",
    "Ориентация $-$ это угол, под которым направлен объект. Следующий метод также дает длины **Major Axis** и **Minor Axis**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:39:51.033135Z",
     "start_time": "2020-07-18T05:39:51.029131Z"
    }
   },
   "outputs": [],
   "source": [
    "(x, y), (MA, ma), angle = cv2.fitEllipse(cnt)\n",
    "\n",
    "print(x, y, MA, ma, angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Маска и пиксельные точки\n",
    "В некоторых случаях нам могут понадобиться все точки, которые составляют этот объект. Это можно сделать следующим образом:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:43:23.166618Z",
     "start_time": "2020-07-18T05:43:23.157068Z"
    }
   },
   "outputs": [],
   "source": [
    "mask = np.zeros(gray_img.shape, np.uint8)\n",
    "cv2.drawContours(mask, [cnt], 0, 255, -1)\n",
    "pixelpoints = np.transpose(np.nonzero(mask))\n",
    "print(f'Numpy shape: {pixelpoints.shape}')\n",
    "\n",
    "pixelpoints = cv2.findNonZero(mask)\n",
    "print(f'CV2 shape: {pixelpoints.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь два метода, один из которых использует функции Numpy, а другой $-$ функцию OpenCV (последняя закомментированная строка), дают то же самое. Результаты тоже такие же, но с небольшой разницей. Numpy дает координаты в формате **(строка, столбец)**, а OpenCV - в формате **(x, y)**. Так что в основном ответы будут взаимозаменяемы. Обратите внимание, что row = x и column = y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Максимальное значение, минимальное значение и их местоположение\n",
    "Мы можем найти эти параметры, используя изображение маски."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:43:52.939145Z",
     "start_time": "2020-07-18T05:43:52.934161Z"
    }
   },
   "outputs": [],
   "source": [
    "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(gray_img, mask = mask)\n",
    "\n",
    "print(min_val, max_val, min_loc, max_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Средний цвет или средняя интенсивность\n",
    "Здесь мы можем найти средний цвет объекта. Или это может быть средняя интенсивность объекта в режиме градаций серого. Мы снова используем ту же маску, чтобы сделать это.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:43:53.588412Z",
     "start_time": "2020-07-18T05:43:53.584398Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_val = cv2.mean(gray_img, mask = mask)\n",
    "\n",
    "print(mean_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Экстремальные точки\n",
    "Экстремальные точки означают самые верхние, самые нижние, самые правые и самые левые точки объекта.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:44:14.239828Z",
     "start_time": "2020-07-18T05:44:14.235224Z"
    }
   },
   "outputs": [],
   "source": [
    "leftmost = tuple(cnt[cnt[:, :, 0].argmin()][0])\n",
    "rightmost = tuple(cnt[cnt[:, :, 0].argmax()][0])\n",
    "topmost = tuple(cnt[cnt[:, :, 1].argmin()][0])\n",
    "bottommost = tuple(cnt[cnt[:, :, 1].argmax()][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:44:16.346776Z",
     "start_time": "2020-07-18T05:44:16.161638Z"
    }
   },
   "outputs": [],
   "source": [
    "print(leftmost, rightmost,'\\n', topmost, bottommost)\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.scatter(leftmost[0], leftmost[1])\n",
    "plt.scatter(rightmost[0], rightmost[1])\n",
    "plt.scatter(topmost[0], topmost[1])\n",
    "plt.scatter(bottommost[0], bottommost[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Гистограмма направленных градиентов (HOG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гистограмма ориентированных градиентов, или сокращенно HOG, - это дескрипторы, в основном используемые в компьютерном зрении и машинном обучении для обнаружения объектов. Однако мы также можем использовать дескрипторы HOG для количественного определения и представления как формы, так и текстуры.\n",
    "\n",
    "Функции HOG впервые были представлены Далалом и Триггсом в их статье CVPR 2005 [Histogram of Oriented Gradients for Human Detection.](https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf). В своей работе Далал и Триггс предложили HOG и пятиступенчатый дескриптор для классификации людей на неподвижных изображениях.\n",
    "\n",
    "5 этапов включают в себя:\n",
    "\n",
    "1. Нормализация изображения до описания.\n",
    "2. Вычисление градиентов в обоих направлениях х и у.\n",
    "3. Получение взвешенных голосов в пространственных и ориентационных ячейках.\n",
    "4. Контраст нормализует перекрывающиеся пространственные ячейки.\n",
    "5. Соберите все гистограммы ориентированных градиентов, чтобы сформировать окончательный вектор признаков.\n",
    "\n",
    "Наиболее важными параметрами для дескриптора HOG являются ```orientations```, ```pix_per_cell``` и ```cell_per_block```. Эти три параметра (наряду с размером входного изображения) эффективно контролируют размерность результирующего вектора объектов. Мы рассмотрим эти параметры и их значение позже.\n",
    "\n",
    "В большинстве реальных приложений HOG используется в сочетании с Linear SVM для обнаружения объектов. Причина, по которой HOG используется так интенсивно, заключается в том, что внешний вид и форму локального объекта можно охарактеризовать с помощью распределения локальных градиентов интенсивности. Фактически, это те же самые градиенты изображения, о которых мы узнали на уроке «Градиенты», но только сейчас мы собираемся взять эти градиенты изображения и превратить их в надежный и мощный дескриптор изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 1: Нормализация изображения до описания.\n",
    "\n",
    "Этот шаг нормализации является необязательным, но в некоторых случаях этот шаг может улучшить производительность дескриптора HOG. Есть три основных метода нормализации, которые мы можем рассмотреть:\n",
    "\n",
    "* __Нормализация гамма/степенного закона.__ \n",
    "\n",
    "    В этом случае мы берем $\\log(p)$ каждого пикселя $p$ во входном изображении. Однако, как показали Далал и Триггс, такой подход, возможно, является «чрезмерной коррекцией» и снижает производительность.\n",
    "\n",
    "\n",
    "* __Нормализация квадратного корня.__\n",
    "\n",
    "    здесь мы берем $\\sqrt(p)$ каждого пикселя $p$ во входном изображении. По определению нормализация квадратного корня сжимает интенсивности входного пикселя намного меньше, чем нормализация гаммы. И снова, как показали Далал и Триггс, нормализация квадратного корня на самом деле повышает точность, а не вредит ей.\n",
    "\n",
    "\n",
    "* __Нормализация дисперсии.__ \n",
    "    \n",
    "    Здесь мы вычисляем как среднее $\\mu$, так и стандартное отклонение $\\sigma$ входного изображения. Все пиксели отцентрированы по центру путем вычитания среднего значения из интенсивности пикселей, а затем нормализованы путем деления на стандартное отклонение: $p'= (p - \\mu) / \\sigma$.\n",
    "\n",
    "В большинстве случаев лучше начинать без нормализации или с нормализацией квадратного корня. Нормализация дисперсии также заслуживает рассмотрения, но в большинстве случаев она будет работать аналогично нормализации квадратного корня (по крайней мере, по моему опыту)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 2: Вычисление градиента\n",
    "Первым фактическим шагом в дескрипторе HOG является вычисление градиента изображения как в направлении $x$, так и в направлении $y$. Эти расчеты должны казаться знакомыми, так как мы уже рассмотрели их на уроке Градиентов.\n",
    "\n",
    "Мы применим операцию свертки для получения градиентных изображений:\n",
    "\n",
    "$G_{x} = I \\star D_{x}$ и $G_{y} = I \\star D_{y}$\n",
    "где $I$ - входное изображение, $D_{x}$ - наш фильтр в направлении $x$, а $D_{y}$ - наш фильтр в направлении $y$.\n",
    "\n",
    "Для полноты приведем пример вычисления $x$ и $y$ градиента входного изображения:\n",
    "\n",
    "Теперь, когда у нас есть градиентные изображения, мы можем вычислить окончательное представление величины градиента изображения:\n",
    "\n",
    "$|G| = \\sqrt{G_{x}^{2} + G_{y}^{2}}$\n",
    "\n",
    "\n",
    "Наконец, ориентация градиента для каждого пикселя во входном изображении может быть рассчитана следующим образом:\n",
    "\n",
    "$\\theta = arctan2(G_{y}, G_{x})$\n",
    "Учитывая оба $|G|$ и $\\theta$, теперь мы можем вычислить гистограмму ориентированных градиентов, где ячейка гистограммы основана на $\\theta$, а вклад или вес, добавленный к данной ячейке гистограммы, основан на $|G|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 3: Взвешенные голоса в каждой ячейке\n",
    "Теперь, когда у нас есть значения градиента и ориентации, нам нужно разделить наше изображение на ячейки и блоки.\n",
    "\n",
    "«Ячейка» - это прямоугольная область, определяемая количеством пикселей, принадлежащих каждой ячейке. Например, если бы у нас было изображение размером 128 x 128 и мы определили бы нашу ```px_per_cell``` как 4 x 4, у нас было бы 32 x 32 = 1024 ячеек.\n",
    "\n",
    "<img src=\"img/hog_4x4.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Если бы мы определили нашу px_per_cell как 32 x 32, у нас было бы всего 4 x 4 = 16 ячеек.\n",
    "\n",
    "<img src=\"img/hog_32x32.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "И если бы мы определили пиксель_пер_клетка равным 128 х 128, у нас была бы всего 1 ячейка.\n",
    "\n",
    "<img src=\"img/hog_128x128.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Очевидно, это довольно преувеличенный пример; мы, вероятно, никогда не были бы заинтересованы в представлении 1 x 1 ячейки. Вместо этого это демонстрирует, как мы можем разделить изображение на ячейки на основе количества пикселей на ячейку.\n",
    "\n",
    "Теперь для каждой из ячеек на изображении нам нужно построить гистограмму ориентированных градиентов, используя нашу величину градиента $|G|$ и ориентация $\\theta$, упомянутая выше.\n",
    "\n",
    "Но прежде чем мы построим эту гистограмму, нам нужно определить число наших ориентаций. Количество ориентаций управляет количеством бинов в итоговой гистограмме. Угол градиента находится в диапазоне $[0, 180]$ (без знака) или $[0, 360]$ (со знаком). В общем, предпочтительно использовать градиенты без знака в диапазоне $[0, 180]$ с ориентациями где-то в диапазоне $[9, 12]$. Но в зависимости от вашего приложения использование градиентов со знаком вместо градиентов без знака может повысить точность.\n",
    "\n",
    "Наконец, каждый пиксель вносит взвешенный голос в гистограмму; вес голоса - это просто величина градиента $|G|$ в данном пикселе.\n",
    "\n",
    "<img src=\"img/hog_histogram_animation.gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "На этом этапе мы можем собрать и объединить каждую из этих гистограмм, чтобы сформировать наш конечный вектор признаков. Тем не менее, полезно применить нормализацию блоков, о которой мы рассмотрим в следующем разделе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 4: Нормализация контраста по блокам\n",
    "Чтобы учесть изменения освещенности и контраста, мы можем нормализовать значения градиента локально. Это требует группировки «ячеек» вместе в более крупные, соединяющие «блоки». Обычно эти блоки перекрываются, что означает, что каждая ячейка вносит свой вклад в конечный вектор признаков более одного раза.\n",
    "\n",
    "Опять же, количество блоков прямоугольное; однако наши единицы больше не являются пикселями - они являются ячейками! Далал и Триггс сообщают, что использование 2 x 2 или 3 x 3 ```cell_per_block``` в большинстве случаев дает разумную точность.\n",
    "\n",
    "<img src=\"img/hog_contrast_normalization.gif\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Для каждой из ячеек в текущем блоке мы объединяем их соответствующие градиентные гистограммы, после чего следуем либо L1, либо L2, нормализуя весь каскадный вектор признаков. Опять же, выполнение этого типа нормализации подразумевает, что каждая из ячеек будет представлена в конечном векторе признаков несколько раз, но нормализована другим значением. Хотя это многократное представление является избыточным и расточительным, оно фактически увеличивает производительность дескриптора.\n",
    "\n",
    "Наконец, после нормализации всех блоков мы берем полученные гистограммы, объединяем их и рассматриваем их как наш конечный вектор признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация\n",
    "\n",
    "Вот пример того, как вычислить дескрипторы HOG, используя ```scikit-image```:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```skimage.feature.hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3), block_norm='L2-Hys', visualize=False, transform_sqrt=False, feature_vector=True, multichannel=None)```\n",
    "\n",
    "**Параметры**\n",
    "* image(M, N[, C]) ndarray\n",
    "        Входное изображение\n",
    "\n",
    "* orientationsint, optional\n",
    "        Количество bins ориентации\n",
    "\n",
    "* pixels_per_cell2-tuple (int, int), optional\n",
    "        Размер (в пикселях) ячейки\n",
    "\n",
    "* cells_per_block2-tuple (int, int), optional\n",
    "        Количество ячеек в каждом блоке\n",
    "\n",
    "* block_normstr {‘L1’, ‘L1-sqrt’, ‘L2’, ‘L2-Hys’}, optional\n",
    "        Метод нормализации блока:\n",
    "        1. L1\n",
    "        Нормализация с использованием L1-нормы\n",
    "\n",
    "        2. L1-sqrt\n",
    "        Нормализация с использованием L1-нормы с последующим квадратным корнем.\n",
    "\n",
    "        3. L2\n",
    "        Нормализация с использованием L2-нормы.\n",
    "\n",
    "        4. L2-Hys\n",
    "        Нормализация с использованием L2-нормы с последующим ограничением максимальных значений до 0,2 (Hys обозначает гистерезис) и перенормировка с использованием L2-нормы. (дефолт)\n",
    "\n",
    "* visualizebool, optional\n",
    "        Также вернет изображение HOG. Для каждой ячейки и ячейки ориентации изображение содержит линейный сегмент, который центрируется в центре ячейки, перпендикулярен средней точке диапазона углов, охватываемых ячейкой ориентации, и имеет интенсивность, пропорциональную соответствующему значению гистограммы.\n",
    "\n",
    "* transform_sqrtbool, optional\n",
    "        Применить степенное сжатие, чтобы нормализовать изображение перед обработкой. НЕ используйте это, если изображение содержит отрицательные значения.\n",
    "\n",
    "* feature_vectorbool, optional\n",
    "        Верните данные как вектор объектов, вызвав .ravel() для результата непосредственно перед возвратом.\n",
    "\n",
    "* multichannelboolean, optional\n",
    "        Если True, последнее измерение изображения рассматривается как цветовой канал, иначе как пространственный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:44:29.449301Z",
     "start_time": "2020-07-18T05:44:29.445891Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage import feature\n",
    "from skimage import exposure\n",
    "from skimage import feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:44:39.036375Z",
     "start_time": "2020-07-18T05:44:39.028796Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/RGB_cube.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:44:43.557536Z",
     "start_time": "2020-07-18T05:44:39.608719Z"
    }
   },
   "outputs": [],
   "source": [
    "(H, hogImage) = feature.hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                            cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\",\n",
    "                            visualize=True)\n",
    "\n",
    "hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255))\n",
    "hogImage = hogImage.astype(\"uint8\")\n",
    "\n",
    "cv2.imshow(\"HOG Image\", hogImage)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
