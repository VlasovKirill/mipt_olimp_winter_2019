{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция №5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Особые точки\n",
    "\n",
    "Настало время перейти к более продвинутым методам сравнения изображений. На прошлых занятих мы рассматривали методы, которые базировались больше на глобальных признаках изображения. Однако, для того, чтобы исследовать схожесть неких изображений, недостаточно иметь представление только глобальных признаков. В качестве шага в определение локальных признаков мы рассматривали метод \"плавающего окна\". \n",
    "\n",
    "Итак, введем предположение, что на изображении можно найти локальные признаки в некой окрестности __ключевой точки__. Вопрос: что брать за ключевую точку и какую окрестность?\n",
    "\n",
    "Для начала разберемся почему глобальные признаки иногда работают плохо на примере следующих изображений.\n",
    "\n",
    "<img src=\"https://i.ibb.co/6FFHWd5/image--000.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "\n",
    "Из всего этого вытекает идея - хочется как-то выделить точки их окрестности, к которым можно привязаться для сравнения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свойства особых точек\n",
    "1. Их должно быть немного\n",
    "    \n",
    "    • существенно меньше, чем пикселей на изображении\n",
    "    \n",
    "    \n",
    "2. Информативные, репрезентативные, уникальные (distinctive)\n",
    "   \n",
    "   • Если окрестности двух точек не отличимы, будет сложно понять, какую из них сопоставить искомому фрагменту\n",
    "   \n",
    "\n",
    "3. Повторяемые (repeatable)\n",
    "    \n",
    "    • Одна и та же точка должна находиться на изображении вне зависимости от геометрических и фотометрических изменений объекта съемки\n",
    "    \n",
    "\n",
    "4. Локальные (local)\n",
    "   \n",
    "   • Небольшого размера, устойчивы к частичному перекрытию другим объектом\n",
    "   \n",
    "## Повторяемость особых точек\n",
    "    ● Необходимо, чтобы хотя бы часть особых точек первого изображения была обнаружена на втором\n",
    "\n",
    "    ● При этом обнаружение особых точек должно происходить независимо для каждого изображения\n",
    "\n",
    "## Информативность, репрезентативность\n",
    "    ● Желательна однозначность в сопоставлении фрагментов\n",
    "    \n",
    "    ● Желательна инвариантность к геометрическим и фотометрическим трансформациям объекта на разных изображениях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Геометрические и фотометрические трансформации изображения.\n",
    "\n",
    "● Геометрические:\n",
    "\n",
    "    ○ Поворот\n",
    "    \n",
    "    ○ Поворот + изменение масштаба\n",
    "    \n",
    "    ○ Афинные преобразования\n",
    "    \n",
    "● Фотометрические\n",
    "\n",
    "    ○ Афинные преобразования интенстивности (I → a I + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение изображений при помощи локальных признаков: основные шаги\n",
    "\n",
    "1. Локализация особых точек\n",
    "\n",
    "\n",
    "2. Выделение особых фрагментов – окрестности ключевых точек, инвариантные к различного рода преобразованиям\n",
    "\n",
    "\n",
    "3. Построение векторов признаков для найденных фрагментов\n",
    "\n",
    "\n",
    "4. Сопоставление наборов локальных признаков для двух изображений\n",
    "    \n",
    "<img src=\"https://i.ibb.co/fD6fRDJ/image--007.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем к рассмотрению популярных дексрипторов изображений для особых точек. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harris Corner Detection\n",
    "\n",
    "В предыдущей главе мы увидели, что углы - это области изображения с большой разницей в интенсивности во всех направлениях. Одна из ранних попыток найти эти углы была сделана Крисом Харрисом и Майком Стивенсом в их статье «Комбинированный детектор углов и краев» в 1988 году, так что теперь она называется «Детектор углов Харриса». Он перенес эту простую идею в математическую форму. Это в основном находит разницу в интенсивности для смещения (u, v) во всех направлениях. Это выражается как ниже:\n",
    "\n",
    "$$E(u,v) = \\sum_{x,y} \\underbrace{w(x,y)}_\\text{window function} \\, [\\underbrace{I(x+u,y+v)}_\\text{shifted intensity}-\\underbrace{I(x,y)}_\\text{intensity}]^2$$\n",
    "\n",
    "Оконная функция - это либо прямоугольное окно, либо гауссово окно, которое дает вес пикселям внизу.\n",
    "\n",
    "Мы должны максимизировать эту функцию E (u, v) для обнаружения угла. Это означает, что мы должны максимизировать второй срок. Применяя расширение Тейлора к вышеприведенному уравнению и используя некоторые математические шаги (пожалуйста, обратитесь к любым стандартным учебникам, которые вам нравятся для полного вывода), мы получаем окончательное уравнение как:\n",
    "\n",
    "$$E(u,v) \\approx \\begin{bmatrix} u & v \\end{bmatrix} M \\begin{bmatrix} u \\\\ v \\end{bmatrix}$$\n",
    "\n",
    "где\n",
    "\n",
    "$$M = \\sum_{x,y} w(x,y) \\begin{bmatrix}I_x I_x & I_x I_y \\\\\n",
    "                                     I_x I_y & I_y I_y \\end{bmatrix}$$\n",
    "\n",
    "Здесь $I_x$ и $I_y$ являются производными изображения в направлениях $x$ и $y$ соответственно. (Может быть легко обнаружен с помощью cv2.Sobel()).\n",
    "\n",
    "Затем идет основная часть. После этого они создали счет, в основном уравнение, которое определит, может ли окно содержать угол или нет.\n",
    "\n",
    "$$R = det(M) - k(trace(M))^2$$\n",
    "\n",
    "где\n",
    "\n",
    "1. $det (M) = \\lambda_1 \\lambda_2$\n",
    "\n",
    "2. $trace (M) = \\lambda_1 + \\lambda_2$\n",
    "\n",
    "3. $\\lambda_1$ и $\\lambda_2$ - собственные значения $M$\n",
    "\n",
    "Таким образом, значения этих собственных значений определяют, является ли область угловой, кромочной или плоской.\n",
    "\n",
    "1. Когда $|R|$ маленький, что случается, когда $\\lambda_1$ и $\\lambda_2$ маленькие, область плоская.\n",
    "\n",
    "2. Когда $R<0$, что происходит, когда $\\lambda_1 >> \\lambda_2$ или наоборот, область является ребром.\n",
    "\n",
    "3. Когда $R$ большое, что происходит, когда $\\lambda_1$ и $\\lambda_2$ большие и $\\lambda_1 \\sim \\lambda_2$, регион является углом.\n",
    "\n",
    "На картинке это можно представить следующим образом:\n",
    "<img src=\"https://i.ibb.co/3CvVLTP/image--017.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harris Corner Detector in OpenCV\n",
    "Для этой цели в OpenCV есть функция ```cv2.cornerHarris(img, blockSize, ksize, k)```. Его аргументы:\n",
    "\n",
    "1. **img** - входное изображение, оно должно быть в градациях серого и иметь тип float32.\n",
    "2. **blockSize** - это размер окрестности, рассматриваемый для обнаружения угла\n",
    "3. **ksize** - параметр апертуры используемого производного Собеля.\n",
    "4. **k** - свободный параметр детектора Харриса в уравнении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T06:31:35.958939Z",
     "start_time": "2020-07-21T06:31:35.648678Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T06:31:35.982795Z",
     "start_time": "2020-07-21T06:31:35.833Z"
    }
   },
   "outputs": [],
   "source": [
    "fname = 'img/lk.jpg'\n",
    "img = cv2.imread(fname)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray = np.float32(gray)\n",
    "dst = cv2.cornerHarris(gray, blockSize=3, ksize=1, k=0.14)\n",
    "\n",
    "# result is dilated for marking the corners, not important\n",
    "dst = cv2.dilate(dst, None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img[dst > 0.10*dst.max()] = [0, 0, 255]\n",
    "\n",
    "# cv2.namedWindow(\"Corners\", cv2.WINDOW_NORMAL) \n",
    "cv2.startWindowThread()\n",
    "# img = cv2.resize(img (960, 540))      \n",
    "cv2.imshow('Corners', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T18:52:56.734667Z",
     "start_time": "2020-07-20T18:52:40.938215Z"
    }
   },
   "outputs": [],
   "source": [
    "source_window = 'Source image'\n",
    "corners_window = 'Corners detected'\n",
    "max_thresh = 255\n",
    "\n",
    "def cornerHarris_demo(val):\n",
    "    thresh = val\n",
    "    # Detector parameters\n",
    "    blockSize = 3\n",
    "    apertureSize = 1\n",
    "    k = 0.14\n",
    "    # Detecting corners\n",
    "    dst = cv2.cornerHarris(src_gray, blockSize, apertureSize, k)\n",
    "    \n",
    "    # Normalizing\n",
    "    dst_norm = np.empty(dst.shape, dtype=np.float32)\n",
    "    cv2.normalize(dst, dst_norm, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    dst_norm_scaled = cv2.convertScaleAbs(dst_norm)\n",
    "    \n",
    "    # Drawing a circle around corners\n",
    "    for i in range(dst_norm.shape[0]):\n",
    "        for j in range(dst_norm.shape[1]):\n",
    "            if int(dst_norm[i,j]) > thresh:\n",
    "                cv2.circle(dst_norm_scaled, (j,i), 5, (0), 2)\n",
    "    \n",
    "    # Showing the result\n",
    "    cv2.namedWindow(corners_window)\n",
    "    cv2.startWindowThread()\n",
    "    cv2.imshow(corners_window, dst_norm_scaled)\n",
    "\n",
    "filename = 'img/lk.jpg'\n",
    "src = cv2.imread(filename)\n",
    "src_gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while True:\n",
    "    # Create a window and a trackbar\n",
    "    cv2.namedWindow(source_window)\n",
    "    thresh = 200 # initial threshold\n",
    "    cv2.createTrackbar('Threshold: ', source_window, thresh, max_thresh, cornerHarris_demo)\n",
    "    cv2.startWindowThread()\n",
    "    cv2.imshow(source_window, src)\n",
    "\n",
    "    cornerHarris_demo(thresh)\n",
    "\n",
    "    key = cv2.waitKey(5)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shi-Tomasi Corner Detector & Good Features to Track\n",
    "В прошлой главе мы увидели детектор углов Харриса. Позже в 1994 году Дж. Ши и К. Томази внесли небольшую модификацию в свою статью Good Features to Track, которая показывает лучшие результаты по сравнению с детектором Harris Corner Detector. Оценочную функцию в Harris Corner Detector предоставили:\n",
    "\n",
    "$$R = \\lambda_1 \\lambda_2 - k(\\lambda_1+\\lambda_2)^2$$\n",
    "\n",
    "Вместо этого Ши-Томаси предложил:\n",
    "\n",
    "$$R = min(\\lambda_1, \\lambda_2)$$\n",
    "\n",
    "Если оно больше порогового значения, оно рассматривается как угол. Если мы построим его в пространстве $\\lambda_1 - \\lambda_2$, как мы это делали в Harris Corner Detector, мы получим изображение, как показано ниже:\n",
    "\n",
    "<img src=\"https://i.ibb.co/sCBQZVp/shitomasi_space.png\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "\n",
    "Из рисунка видно, что только когда $\\lambda_1$ и $\\lambda_2$ превышают минимальное значение $\\lambda_ {min}$, оно считается углом (зеленая область)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shi-Tomasi Corner Detector in OpenCV\n",
    "\n",
    "OpenCV имеет функцию ```cv2.goodFeaturesToTrack()```. Он находит N самых сильных углов на изображении методом Ши-Томази (или Обнаружение углов Харриса, если вы его укажете). Как обычно, изображение должно быть изображением в градациях серого. Затем вы указываете количество углов, которые вы хотите найти. Затем вы указываете уровень качества, который является значением в диапазоне 0-1, что обозначает минимальное качество угла, ниже которого все отклоняются. Затем мы предоставляем минимальное евклидово расстояние между обнаруженными углами.\n",
    "\n",
    "Со всей этой информацией функция находит углы на изображении. Все углы ниже уровня качества отклоняются. Затем сортирует оставшиеся углы по качеству в порядке убывания. Затем функция занимает первый самый сильный угол, отбрасывает все близлежащие углы в диапазоне минимального расстояния и возвращает N самых сильных углов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-20T13:40:49.298Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/lk.jpg')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Найти углы\n",
    "corners = cv2.goodFeaturesToTrack(gray, 200, 0.1, 30)\n",
    "corners = np.int0(corners)\n",
    "\n",
    "# Отрисуем найденные точки \n",
    "for i in corners:\n",
    "    x,y = i.ravel()\n",
    "    cv2.circle(img, (x,y), 3, 255, -1)\n",
    "\n",
    "    \n",
    "cv2.imshow('Corners', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T13:29:08.178611Z",
     "start_time": "2020-07-20T13:29:02.752765Z"
    }
   },
   "outputs": [],
   "source": [
    "import random as rng\n",
    "\n",
    "source_window = 'Image'\n",
    "maxTrackbar = 300\n",
    "rng.seed(12345)\n",
    "def goodFeaturesToTrack_Demo(val):\n",
    "    maxCorners = max(val, 1)\n",
    "    # Parameters for Shi-Tomasi algorithm\n",
    "    qualityLevel = 0.2\n",
    "    minDistance = 30\n",
    "    blockSize = 3\n",
    "    gradientSize = 5\n",
    "    useHarrisDetector = False\n",
    "    k = 0.14\n",
    "    # Copy the source image\n",
    "    copy = np.copy(src)\n",
    "    # Apply corner detection\n",
    "    corners = cv2.goodFeaturesToTrack(src_gray, maxCorners, qualityLevel, minDistance, None, \\\n",
    "        blockSize=blockSize, gradientSize=gradientSize, useHarrisDetector=useHarrisDetector, k=k)\n",
    "    # Draw corners detected\n",
    "#     print('** Number of corners detected:', corners.shape[0])\n",
    "    radius = 4\n",
    "    for i in range(corners.shape[0]):\n",
    "        cv2.circle(copy, \n",
    "                   (corners[i,0,0], corners[i,0,1]), radius, (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)),\n",
    "                   cv2.FILLED)\n",
    "    # Show what you got\n",
    "    cv2.namedWindow(source_window)\n",
    "    cv2.imshow(source_window, copy)\n",
    "    \n",
    "# Load source image and convert it to gray\n",
    "\n",
    "filename = 'img/lk.jpg'\n",
    "src = cv2.imread(filename)\n",
    "src_gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a window and a trackbar\n",
    "cv2.namedWindow(source_window)\n",
    "maxCorners = 23 # initial threshold\n",
    "cv2.createTrackbar('Corners: ', source_window, maxCorners, maxTrackbar, goodFeaturesToTrack_Demo)\n",
    "cv2.imshow(source_window, src)\n",
    "\n",
    "goodFeaturesToTrack_Demo(maxCorners)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to SIFT (Scale-Invariant Feature Transform)\n",
    "\n",
    "В последних двух главах мы видели некоторые детекторы углов, такие как Харрис и т. Д. Они не зависят от вращения, что означает, что даже если изображение поворачивается, мы можем найти те же углы. Это очевидно, потому что углы остаются и в повернутом изображении. Но как насчет масштабирования? Угол может не быть углом, если изображение масштабировано. Например, проверьте простое изображение ниже. Угол на небольшом изображении в небольшом окне является плоским, когда он увеличен в том же окне. Так что угол Харриса не является масштабно-инвариантным.\n",
    "\n",
    "<img src=\"https://i.ibb.co/N7kR8Lc/sift_scale_invariant.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "Рассмотрим новый алгоритм Scale Invariant Feature Transform (SIFT), который позволяет сохранить инвариантость к масштабированию.\n",
    "\n",
    "## Обнаружение экстремумов в пространстве\n",
    "\n",
    "Из рисунка выше видно, что мы не можем использовать одно и то же окно для обнаружения ключевых точек с разным масштабом. Это нормально с небольшим углом. Но чтобы обнаружить большие углы, нам нужны большие окна. Для этого используется масштабная фильтрация. В нем найден лапласиан гауссовский для изображения с различными значениями $\\sigma$. LoG действует как детектор капель, который обнаруживает капли разных размеров из-за изменения $\\sigma$. Короче говоря, $\\sigma$ действует как параметр масштабирования. Например, на изображении выше, ядро Гаусса с низким значением $\\sigma$ дает высокое значение для небольшого угла, в то время как ядро гауссиана с высоким значением $\\sigma$ хорошо подходит для большего угла. Таким образом, мы можем найти локальные максимумы по шкале и пространству, которые дают нам список значений $(x, y, \\sigma)$, что означает, что существует потенциальная ключевая точка в точке $(x, y)$ в масштабе $\\sigma$.\n",
    "\n",
    "Но этот LoG немного дорог, поэтому алгоритм SIFT использует разность гауссианов, которая является приближением LoG. Разность по Гауссу получается как разница по размытию по Гауссу изображения с двумя разными $\\sigma$, пусть это будут $\\sigma$ и $k \\sigma$. Этот процесс выполняется для разных октав изображения в гауссовой пирамиде. Это представлено на изображении ниже:\n",
    "\n",
    "<img src=\"https://i.ibb.co/WWyXGrD/sift_dog.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Как только эта DoG найдена, изображения ищут локальные экстремумы в масштабе и пространстве. Например, один пиксель в изображении сравнивается с его 8 соседями, а также с 9 пикселями в следующем масштабе и 9 пикселями в предыдущих масштабах. Если это локальный экстремум, это потенциальная ключевая точка. Это в основном означает, что ключевой момент лучше всего представлен в этом масштабе. Это показано на рисунке ниже:\n",
    "\n",
    "<img src=\"https://i.ibb.co/2YN6FZ6/sift_local_extrema.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Что касается различных параметров, в статье приведены некоторые эмпирические данные, которые можно суммировать как: число $octaves = 4$, количество уровней $scale = 5$, начальные $\\sigma = 1.6$, $k = \\sqrt {2}$ и т. Д. В качестве оптимальных значений.\n",
    "\n",
    "## Локализация ключевых точек\n",
    "\n",
    "Как только потенциальные ключевые точки найдены, их необходимо уточнить, чтобы получить более точные результаты. Они использовали расширение масштаба шкалы Тейлора, чтобы получить более точное местоположение экстремумов, и если интенсивность этих экстремумов меньше порогового значения (0,03 согласно статье), оно отклоняется. Этот порог называется контрастным порогом в OpenCV\n",
    "\n",
    "У DoG более высокий отклик на ребра, поэтому ребра также должны быть удалены. Для этого используется концепция, похожая на угловой детектор Harris. Они использовали матрицу Гессиана 2x2 (H) для вычисления кривизны кривизны. Из углового детектора Харриса мы знаем, что для ребер одно собственное значение больше другого. Так что здесь они использовали простую функцию,\n",
    "\n",
    "Если это отношение больше порога, называемого edgeThreshold в OpenCV, эта ключевая точка отбрасывается. Это дано как 10 в статье.\n",
    "\n",
    "Таким образом, он устраняет любые малоконтрастные ключевые точки и граничные ключевые точки, а остающиеся точки - это сильные точки интереса.\n",
    "\n",
    "## Ориентация значения\n",
    "\n",
    "Теперь ориентация назначается каждой ключевой точке для достижения неизменности вращения изображения. Вокруг местоположения ключевой точки берется окрестность в зависимости от масштаба, и в этой области вычисляются величина и направление градиента. Создается гистограмма ориентации с 36 ячейками, охватывающими 360 градусов. (Он взвешивается по величине градиента и по гауссовому круглому окну с $\\sigma$, равным 1,5-кратному масштабу ключевой точки. Принимается самый высокий пик в гистограмме, и любой пик выше 80% также считается для расчета ориентации. Он создает ключевые точки с одинаковым расположением и масштабом, но разными направлениями, что способствует стабильности сопоставления.\n",
    "\n",
    "## Дескриптор ключевой точки\n",
    "\n",
    "Теперь дескриптор ключевой точки создан. Окрестность 16x16 вокруг ключевой точки берется. Он разделен на 16 субблоков размером 4х4. Для каждого субблока создается гистограмма ориентации 8 бинов. Таким образом, доступно 128 значений бина. Он представлен в виде вектора для формирования дескриптора ключевой точки. В дополнение к этому, предприняты некоторые меры для достижения устойчивости к изменениям освещенности, вращению и т. Д.\n",
    "\n",
    "## Сопоставление ключевых точек \n",
    "\n",
    "Ключевые точки между двумя изображениями сопоставляются путем определения ближайших соседей. Но в некоторых случаях второй ближайший матч может быть очень близко к первому. Это может произойти из-за шума или по другим причинам. В этом случае берется отношение ближайшего расстояния ко второму ближайшему расстоянию. Если оно больше 0,8, они отклоняются. Это устраняет около 90% ложных совпадений, в то время как отбрасывает только 5% правильных совпадений, согласно статье.\n",
    "\n",
    "Так что это краткое изложение алгоритма SIFT. Для получения более подробной информации и понимания, чтение оригинальной статьи настоятельно рекомендуется. Помните одну вещь, этот алгоритм запатентован. Таким образом, этот алгоритм включен в модуль Non-free в OpenCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T06:33:01.978679Z",
     "start_time": "2020-07-21T06:33:01.962678Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'img/lk.jpg'\n",
    "src = cv2.imread(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T06:33:03.060456Z",
     "start_time": "2020-07-21T06:33:02.125400Z"
    }
   },
   "outputs": [],
   "source": [
    "src = cv2.imread('img/lk.jpg')\n",
    "\n",
    "# Step 1: Detect the keypoints using SURF Detector\n",
    "detector = cv2.xfeatures2d.SIFT_create()\n",
    "keypoints = detector.detect(src, None)\n",
    "\n",
    "# Draw keypoints\n",
    "img_keypoints = np.empty((src.shape[0], src.shape[1], 3), dtype=np.uint8)\n",
    "cv2.drawKeypoints(src, keypoints, img_keypoints, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Show detected (drawn) keypoints\n",
    "cv2.imshow('SURF Keypoints', img_keypoints)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Поиск объектов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T06:33:04.370347Z",
     "start_time": "2020-07-21T06:33:03.609526Z"
    }
   },
   "outputs": [],
   "source": [
    "img_car = cv2.imread('img/crop_lk.jpg')\n",
    "\n",
    "cv2.imshow('Crop', img_car)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T06:33:04.934518Z",
     "start_time": "2020-07-21T06:33:04.744315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Detect the keypoints using SURF Detector\n",
    "hyp_params = dict(\n",
    "    nfeatures = 0,\n",
    "    nOctaveLayers = 3,\n",
    "    contrastThreshold = 0.03,\n",
    "    edgeThreshold = 10,\n",
    "    sigma = 1.6 )  # hyp params\n",
    "\n",
    "detector = cv2.xfeatures2d.SIFT_create(**hyp_params)\n",
    "\n",
    "# calculate feature points and they descriptors \n",
    "keypoints1, desc1 = detector.detectAndCompute(src, None)\n",
    "keypoints2, desc2 = detector.detectAndCompute(img_car, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T06:33:05.201150Z",
     "start_time": "2020-07-21T06:33:05.197814Z"
    }
   },
   "outputs": [],
   "source": [
    "# init match algo\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T06:33:05.513965Z",
     "start_time": "2020-07-21T06:33:05.481055Z"
    }
   },
   "outputs": [],
   "source": [
    "# knn search\n",
    "matches = flann.knnMatch(desc1, desc2, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T06:33:21.505490Z",
     "start_time": "2020-07-21T06:33:21.502233Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply threshold rule\n",
    "ratio_thresh = 0.3\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < ratio_thresh * n.distance:\n",
    "        good_matches.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T06:33:27.982991Z",
     "start_time": "2020-07-21T06:33:21.830045Z"
    }
   },
   "outputs": [],
   "source": [
    "#-- Draw matches\n",
    "img_matches = np.empty((max(src.shape[0], img_car.shape[0]), src.shape[1] + img_car.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = (255,0,0),\n",
    "                   flags = cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "                  )\n",
    "\n",
    "cv2.drawMatches(src, keypoints1, img_car, keypoints2, good_matches, img_matches, **draw_params)\n",
    "#-- Show detected matches\n",
    "cv2.imshow('Good Matches', img_matches)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
