{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика: Погружение в глубокое обучение\n",
    "В семинаре, будем использовать набор данных `fashion_mnist`, загрузим их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', train = True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', train = False, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(trainset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = trainset.train_data\n",
    "y_train = trainset.train_labels\n",
    "\n",
    "x_test = testset.train_data\n",
    "y_test = testset.train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "for i in range(num_classes):\n",
    "    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n",
    "    idx = np.where(y_train[:]==i)[0]\n",
    "    features_idx = x_train[idx,::]\n",
    "    img_num = np.random.randint(features_idx.shape[0])\n",
    "    im = features_idx[img_num]\n",
    "    ax.set_title(trainset.classes[i])\n",
    "    plt.imshow(im, cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проведем небольшие предобработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat = x_train.flatten(start_dim=1).float()\n",
    "x_test_flat = x_test.flatten(start_dim=1).float()\n",
    "print(f'Была размерность: {x_train.shape}, стала: {x_train_flat.shape}')\n",
    "print(f'Была размерность: {x_test.shape}, стала: {x_test_flat.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_out = num_classes\n",
    "D_in = x_train_flat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, 128),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(128, 10),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(10, D_out),\n",
    "    torch.nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(28 * 28, 200)\n",
    "        self.fc2 = torch.nn.Linear(200, 200)\n",
    "        self.fc3 = torch.nn.Linear(200, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x)\n",
    "model = Model()\n",
    "   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "\n",
    "# Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "# override the __call__ operator so you can call them like functions. When\n",
    "# doing so you pass a Tensor of input data to the Module and it produces\n",
    "# a Tensor of output data.\n",
    "y_pred = model(x_train_flat.float())\n",
    "\n",
    "# Compute and print loss. We pass Tensors containing the predicted and true\n",
    "# values of y, and the loss function returns a Tensor containing the\n",
    "# loss.\n",
    "loss_old = loss_fn(y_pred, y_train)\n",
    "acc_old = accuracy_score(y_train.numpy(), y_pred.argmax(dim=1).numpy())\n",
    "\n",
    "# Zero the gradients before running the backward pass.\n",
    "model.zero_grad()\n",
    "\n",
    "# Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "# parameters of the model. Internally, the parameters of each Module are stored\n",
    "# in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "# all learnable parameters in the model.\n",
    "loss_old.backward()\n",
    "\n",
    "# Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "# we can access its gradients like we did before.\n",
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param -= learning_rate * param.grad\n",
    "\n",
    "y_pred = model(x_train_flat.float())\n",
    "loss_new = loss_fn(y_pred, y_train)\n",
    "step = loss_new.item()-loss_old.item()\n",
    "\n",
    "acc_new = accuracy_score(y_train.numpy(), y_pred.argmax(dim=1).numpy())\n",
    "\n",
    "\n",
    "\n",
    "print(f'Лосс: {loss_old.item()} -> {loss_new.item()}. Step {step} ')\n",
    "print(f'Accuracy: {acc_old} -> {acc_new}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(model, learning_rate, x, y):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    return(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs, batch_size, learning_rate,  X, y, X_test, y_test):\n",
    "    acc_train_all = []\n",
    "    loss_train_all = []\n",
    "    acc_test_all = []\n",
    "    loss_test_all = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        permutation = torch.randperm(X.size()[0])\n",
    "\n",
    "        for i in tqdm(range(0,X.float().size()[0], batch_size)):\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = X[indices], y[indices]\n",
    "            batch_train(model, learning_rate, batch_x, batch_y)\n",
    "\n",
    "        y_test_pred = model(X_test)\n",
    "        y_train_pred = model(X)\n",
    "\n",
    "\n",
    "        acc_train = accuracy_score(y.numpy(), y_train_pred.argmax(dim=1).numpy())\n",
    "        loss_train = loss_fn(y_train_pred, y).detach().numpy() \n",
    "        acc_test = accuracy_score(y_test.numpy(), y_test_pred.argmax(dim=1).numpy())\n",
    "        loss_test = loss_fn(y_test_pred, y_test).detach().numpy()\n",
    "\n",
    "        acc_train_all = np.append(acc_train_all, acc_train)\n",
    "        loss_train_all = np.append(loss_train_all, loss_train)\n",
    "        acc_test_all = np.append(acc_test_all, acc_test)\n",
    "        loss_test_all = np.append(loss_test_all, loss_test)\n",
    "\n",
    "\n",
    "        print(f'Epoch {epoch}: \\n Accuracy - train: {acc_train} | test: {acc_test} \\n Loss - train: {loss_train} | test: {loss_test}')\n",
    "        \n",
    "    return(acc_train_all, loss_train_all, acc_test_all, loss_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 1000 \n",
    "learning_rate = 1e-1\n",
    "\n",
    "acc_train_all, loss_train_all, acc_test_all, loss_test_all = train(model, n_epochs, batch_size, learning_rate, x_train_flat, y_train, x_test_flat, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_history(acc_train_all, loss_train_all, acc_test_all, loss_test_all):\n",
    "    fig = plt.figure(figsize=(16, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    plt.plot(loss_train_all, label='loss')\n",
    "    plt.plot(loss_test_all, label='val_loss')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(acc_train_all, label='acc')\n",
    "    plt.plot(acc_test_all, label='val_acc')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_history(acc_train_all, loss_train_all, acc_test_all, loss_test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание №1:  \n",
    "Измените сеть выше, добавив еще один скрытый слой `Linear()` с 32 нейронами и перезапустите обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что мы можем улучшить? \n",
    "- Отнормировать признаки\n",
    "- Заменить сигмоиды на ReLu\n",
    "- Задать правила инициации весов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормирование\n",
    "<img src='normalize.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "x_train_norm = (x_train_flat/255)*2-1\n",
    "x_test_norm = (x_test_flat/255)*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm.max(), x_train_norm.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции активации\n",
    "<img src='activations.png'>\n",
    "\n",
    "### Инициациия весов\n",
    "__Случайно__  \n",
    "$ w = a * random$, но тогда если $a \\gg 1$, то на выходе $b\\gg1$ и если $a \\ll 1 $, то $b \\approx 0 $  \n",
    "\n",
    "__Xavier__  \n",
    "$a = \\frac{1}{\\sqrt{n}}$, где $n$ - кол-во нейронов на входе\n",
    "\n",
    "__He__  \n",
    "$a = \\frac{1}{\\sqrt{\\frac{n}{2}}}$, где $n$ - кол-во нейронов на входе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание №2:   \n",
    "\n",
    "1. Создайте новую сеть `model_2`:\n",
    " - 128 нейронов с функцией активации ReLU: \n",
    " - 64 нейрона  с функцией активации ReLU:\n",
    " - 32 нейрона с функцией активации ReLU:\n",
    " - Softmax выход\n",
    "\n",
    "2. Примените к модели  функцию инициации весов с помощью метода .apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 =  # Ваш Код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 2028 \n",
    "learning_rate = 1e-2\n",
    "\n",
    "acc_train_all, loss_train_all, acc_test_all, loss_test_all = train(model_2, n_epochs, batch_size, learning_rate,\n",
    "                                                                   x_train_norm, y_train, x_test_norm, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_history(acc_train_all, loss_train_all, acc_test_all, loss_test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Влияние скорости обучения\n",
    "Посмотрим, как влияет параметр `learning_rate` на качество нашей модели на обучающей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e+1, 1e-2, 1e-3, 1e-5, 1e-10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_loss_train = {}\n",
    "batch_size = 5000\n",
    "\n",
    "for i in learning_rates:\n",
    "    model_2.apply(init_weights)\n",
    "    acc_train_all, loss_train_all, acc_test_all, loss_test_all = train(model_2, 25, batch_size, i,\n",
    "                                                                       x_train_flat, y_train, x_test_flat, y_test)\n",
    "    voc_loss_train[i] = loss_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "\n",
    "for i in voc_loss_train.keys():\n",
    "    plt.plot(voc_loss_train[i], label=f'{i}')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Влияние метода оптимизации градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='optimizers7.gif'>\n",
    "\n",
    "#### Momentum\n",
    "Вместо того, чтобы использовать только градиент текущего шага, мы будем накапливать импульс градиента прошлых шагов для определения направления движения. \n",
    "В связи со стохастической природой, обновления градиента происходят \"зигзагообразно\", с помощью момента мы усиливаем движение вдоль основного направления. На практике коэффициент у момента инициализируется на уровне 0,5 и постепенно увеличивается до 0,9 в течение нескольких эпох. \n",
    "  \n",
    "#### RMSProp (Root Mean Square Propogation)   \n",
    "Мы обновляяем меньше веса, которые слишком часто обновляются, и будем использовать усреднённый по истории квадрат градиента.\n",
    "\n",
    "#### Adam (Adaptive moment estimation)\n",
    "Cочетает в себе и идею накопления движения и идею более слабого обновления весов для типичных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model_2.parameters(), lr=0.001, momentum=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(model, x, y):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    #    with torch.no_grad():\n",
    "    #    for param in model.parameters():\n",
    "    #        param -= learning_rate * param.grad\n",
    "    optimizer.step()\n",
    "    return(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "model_2.apply(init_weights)\n",
    "loss_train_sgd = []\n",
    "\n",
    "optimizer = torch.optim.SGD(model_2.parameters(), lr=0.001, momentum=0.0)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    permutation = torch.randperm(x_train_norm.size()[0])\n",
    "\n",
    "    for i in tqdm(range(0,x_train_norm.float().size()[0], batch_size)):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x_train_norm[indices], y_train[indices]\n",
    "        batch_train(model_2, batch_x, batch_y)\n",
    "\n",
    "    y_test_pred = model_2(x_test_norm)\n",
    "    loss_train = loss_fn(y_test_pred, y_test).detach().numpy()\n",
    "    print(f'Epoch: {epoch} loss {loss_train}')\n",
    "    loss_train_sgd = np.append(loss_train_sgd, loss_train)\n",
    "\n",
    "    \n",
    "model_2.apply(init_weights)\n",
    "loss_train_sgd_moment = []\n",
    "\n",
    "optimizer = torch.optim.SGD(model_2.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    permutation = torch.randperm(x_train_norm.size()[0])\n",
    "\n",
    "    for i in tqdm(range(0,x_train_norm.float().size()[0], batch_size)):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x_train_norm[indices], y_train[indices]\n",
    "        batch_train(model_2, batch_x, batch_y)\n",
    "\n",
    "    y_test_pred = model_2(x_test_norm)\n",
    "    loss_train = loss_fn(y_test_pred, y_test).detach().numpy()\n",
    "    print(f'Epoch: {epoch} loss {loss_train}')\n",
    "    loss_train_sgd_moment = np.append(loss_train_sgd_moment, loss_train)\n",
    "    \n",
    "    \n",
    "\n",
    "model_2.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), lr=0.001)\n",
    "loss_train_adam = []\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    permutation = torch.randperm(x_train_norm.size()[0])\n",
    "\n",
    "    for i in tqdm(range(0,x_train_norm.float().size()[0], batch_size)):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x_train_norm[indices], y_train[indices]\n",
    "        batch_train(model_2, batch_x, batch_y)\n",
    "\n",
    "    y_test_pred = model_2(x_test_norm)\n",
    "    loss_train = loss_fn(y_test_pred, y_test).detach().numpy()\n",
    "    print(f'Epoch: {epoch} loss {loss_train}')\n",
    "    loss_train_adam = np.append(loss_train_all, loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.plot(loss_train_sgd, label='SGD')\n",
    "plt.plot(loss_train_sgd_moment, label='SGD with momentum')\n",
    "plt.plot(loss_train_adam, label='Adam')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ссылки\n",
    "- [Курс \"Deep learning на пальцах\", лекция 4](https://youtu.be/tnrbx7V9RbA)\n",
    "- [Статья: Оптимизация градиентного спуска](http://ruder.io/optimizing-gradient-descent/)\n",
    "- [Статья: Методы оптимизации нейронных сетей](https://habr.com/ru/post/318970/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
