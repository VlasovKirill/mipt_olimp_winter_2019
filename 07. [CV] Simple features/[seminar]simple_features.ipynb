{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Свертки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основы сверток\n",
    "**Свертки** $-$ это метод общей обработки сигналов. Люди, изучающие электронику, скажут вам о почти бесконечных бессонных ночах, которые были потрачены на их постижение. На эту тему написано множество книг. Но для компьютерного зрения мы просто разберемся с некоторыми простыми вещами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение\n",
    "Во-первых, давайте посмотрим на математическое определение свертки в дискретной области времени. Позже мы пройдемся по тому, что говорит нам это уравнение.\n",
    "${\\begin{align}y[n]&=x[n]*h[n]=\\sum_{k=-\\infty}^\\infty x[k]\\cdot h[n-k]\\end{align},}$\n",
    "\n",
    "где:\n",
    "* **x[n]** $-$ входой сигнал;\n",
    "* **h[n]** $-$ импульсный отклик; \n",
    "* **y[n]** $-$ выходной сигнал;\n",
    "* **$*$** $-$ обозначает свертку. \n",
    "\n",
    "Обратите внимание, что мы умножаем слагаемые $x[k]$ на смещенные по времени $h[n]$ и складываем их.\n",
    "Краеугольный камень понимания свертки лежит в основе импульсного отклика и импульсного разложения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разложение импульсной функции\n",
    "\n",
    "Чтобы понять смысл свертки, мы собираемся начать с концепции разложения сигналов. Входной сигнал разлагается на простые аддитивные компоненты, и системный отклик входного сигнала приводит к сложению выходных данных этих компонентов, прошедших через систему.\n",
    "\n",
    "В общем, сигнал может быть разложен как взвешенная сумма базовых сигналов. Например, в рядах Фурье любой периодический сигнал (даже прямоугольный импульсный сигнал) может быть представлен суммой функций синуса и косинуса. Но здесь мы используем импульсные (дельта) функции для базовых сигналов, а не синус и косинус.\n",
    "\n",
    "Изучим следующий пример того, как сигнал разлагается на набор импульсных (дельта) функций:\n",
    "<img src=\"https://i.ibb.co/CKQ5sKh/conv_img01.png\" alt=\"Drawing\" style=\"width: 200px;\"/> \n",
    "\n",
    "Импульсная функция выглядит так: ${\\delta(n)={\\begin{cases}1, n=0\\\\ 0, n\\ne0 \\end{cases}}}$.  Тогда для данного примера $x[0]$ можно записать в виде ${x[0] = x[0] \\cdot \\delta[n-0] = 2 \\cdot \\delta[n-0]}$. Тогда $x[1]$ будет равен ${x[1] = x[1] \\cdot \\delta[n-1] = 3 \\cdot \\delta[n-1]}$, потому что ${\\delta[n-1]}$ равна $1$ при $n = 1$ и ноль в других случаях. Таким же образом мы можем записать ${x[2] = x[2] \\cdot \\delta[n-2] = 1 \\cdot \\delta[n-2]}$, сдвинув ${\\delta[n]}$ на 2. Следовательно, сигнал $x[n]$ может быть представлен добавлением $3$ сдвинутых и масштабированных импульсных функций.\n",
    "\n",
    "В общем, сигнал может быть записан как сумма масштабированных и сдвинутых дельта-функций:\n",
    "${\\begin{align}x[n]=\\sum_{k=0}^2 x[k]\\cdot\\delta[n-k]=x[0]\\cdot\\delta[n-0]+x[1]\\cdot\\delta[n-1]+x[2]\\cdot\\delta[n-2]\\end{align}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импульсивный отклик\n",
    "**Импульсный отклик** $-$ это выход системы, являющийся результатом импульсной функции в качестве входа.\n",
    "Именно это обозначается за $h[n]$. Все волшебство именно тут. Для вас это перестанет быть магией на 2-3 курсе инстиута. \n",
    "\n",
    "Существует огромное множество преобразования для импульсного отклика. Мы рассмотрим только конечные формулы без понимания того, как работает математика.\n",
    "\n",
    "<td> <img src=\"https://i.ibb.co/LNJ9h4L/conv_img02.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свертка в 2D\n",
    "2D свертка является просто продолжением предыдущей 1D свертки путем свертки как горизонтального, так и вертикального направлений в двухмерной пространственной области. Свертка часто используется для обработки изображений, таких как сглаживание, повышение резкости и обнаружение краев изображений.\n",
    "\n",
    "Импульсная (дельта) функция также находится в двумерном пространстве, поэтому ${\\delta[m,n]={\\begin{cases}1,\\ m=n=0\\\\0,\\ m\\ne0,n\\ne0\\end{cases}}}$. Импульсный отклик в 2D обычно называют **«ядром»** или **«фильтром»** в обработке изображений.\n",
    "\n",
    "<table><tr>\n",
    "    <td> <img src=\"https://i.ibb.co/RQZ0p5S/conv2d_delta.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "    <td> <img src=\"https://i.ibb.co/nCJMYR7/conv_img10.png\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Кроме того, выходные данные линейной и инвариантной по времени системы могут быть записаны путем свертки входного сигнала $x[m,n]$ и импульсной характеристики $h[m,n]$:\n",
    "${\\begin{align}y[m,n]=x[m,n]*h[m,n]=\\sum_{j=-\\infty}^\\infty \\sum_{i=-\\infty}^\\infty x[i,j]\\cdot h[m-i,n-j]\\end{align}}$\n",
    "\n",
    "Изучим пример, чтобы уточнить, как работает свертка в **2D**-пространстве.\n",
    "Допустим, что размер импульсного отклика (ядра) составляет $3\\times3$, а его значения **a, b, c, d, ...**\n",
    "\n",
    "Обратите внимание, что начало координат $(0,0)$ находится в центре ядра.\n",
    "\n",
    "Давайте выберем простейший пример и вычислим свертку, например, результат в $(1,1)$:\n",
    "<img src=\"https://i.ibb.co/K9mCCgy/conv_img11.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "${\\begin{align}\n",
    "y[1,1] & = \\sum_{j=0}^2 \\sum_{i=0}^2 x[i,j] \\cdot h[1-i,1-j]  \\\\ &\n",
    "= x[0,0] \\cdot h[1,1] + x[1,0] \\cdot h[0,1] + x[2,0] \\cdot h[-1,1] \\\\ &\n",
    "+ x[0,1] \\cdot h[1,0] + x[1,1] \\cdot h[0,0] + x[2,1] \\cdot h[-1,0] \\\\ &\n",
    "+ x[0,2] \\cdot h[1,-1] + x[1,2] \\cdot h[0,-1] + x[2,2] \\cdot h[-1,-1]\n",
    "\\end{align}}$\n",
    "\n",
    "Рассмотрим более конкртеный пример с двумерной сверткой. Предположим, у нас есть входные матрицы $3\\times3$ и ядра $3\\times3$ следующим образом:\n",
    "\n",
    "Input | Kernel | Output\n",
    "-|-|-\n",
    "<img src=\"https://i.ibb.co/1ZqbtRp/conv_img15.png\" alt=\"Drawing\" style=\"width: 200px;\"/> | <img src=\"https://i.ibb.co/mybNNbh/conv_img16.png\" alt=\"Drawing\" style=\"width: 200px;\"/> | <img src=\"https://i.ibb.co/hZKST86/conv_img17.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "Выходной сигнал в точке $(1,1)$ для этого примера будет:\n",
    "${\\begin{align}\n",
    "\\ y[1,1] & = \\sum_{j=0}^2 \\sum_{i=0}^2 x[i,j] \\cdot h[1-i,1-j]  \\\\ &\n",
    "\\ = x[0,0] \\cdot h[1,1] + x[1,0] \\cdot h[0,1] + x[2,0] \\cdot h[-1,1] \\\\ &\n",
    "\\ + x[0,1] \\cdot h[1,0] + x[1,1] \\cdot h[0,0] + x[2,1] \\cdot h[-1,0] \\\\ &\n",
    "\\ + x[0,2] \\cdot h[1,-1] + x[1,2] \\cdot h[0,-1] + x[2,2] \\cdot h[-1,-1] \\\\&\n",
    "\\ = 1 \\cdot 1 + 2 \\cdot 2 + 3 \\cdot 1 \\\\ &\n",
    "\\ + 4 \\cdot 0 + 5 \\cdot 0 + 6 \\cdot 0 \\\\ &\n",
    "\\ + 7 \\cdot (-1) + 8 \\cdot (-2) + 9 \\cdot (-1) \\\\ &\n",
    "\\ = -24 \\\\\n",
    "\\end{align}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution animations\n",
    "\n",
    "_N.B.: Blue maps are inputs, and cyan maps are outputs._\n",
    "\n",
    "<table style=\"width:100%; table-layout:fixed;\">\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"gif/no_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/arbitrary_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/same_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/full_padding_no_strides.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, no strides</td>\n",
    "    <td>Arbitrary padding, no strides</td>\n",
    "    <td>Half padding, no strides</td>\n",
    "    <td>Full padding, no strides</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"gif/no_padding_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/padding_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/padding_strides_odd.gif\"></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, strides</td>\n",
    "    <td>Padding, strides</td>\n",
    "    <td>Padding, strides (odd)</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "## Transposed convolution animations\n",
    "\n",
    "_N.B.: Blue maps are inputs, and cyan maps are outputs._\n",
    "\n",
    "<table style=\"width:100%; table-layout:fixed;\">\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"gif/no_padding_no_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/arbitrary_padding_no_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/same_padding_no_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/full_padding_no_strides_transposed.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, no strides, transposed</td>\n",
    "    <td>Arbitrary padding, no strides, transposed</td>\n",
    "    <td>Half padding, no strides, transposed</td>\n",
    "    <td>Full padding, no strides, transposed</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"gif/no_padding_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/padding_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/padding_strides_odd_transposed.gif\"></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, strides, transposed</td>\n",
    "    <td>Padding, strides, transposed</td>\n",
    "    <td>Padding, strides, transposed (odd)</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "## Dilated convolution animations\n",
    "\n",
    "_N.B.: Blue maps are inputs, and cyan maps are outputs._\n",
    "\n",
    "<table style=\"width:25%\"; table-layout:fixed;>\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"gif/dilation.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, no stride, dilation</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "*Источник: https://github.com/vdumoulin/conv_arithmetic*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Свертки в OpenCV (Фильтрация изображений)\n",
    "\n",
    "Как и в одномерных сигналах, изображения также могут быть отфильтрованы с помощью различных низкочастотных фильтров (LPF), высокочастотных фильтров (HPF) и т.д. \n",
    "**LPF** помогает удалять шумы, размывать изображения. Фильтры **HPF** помогают находить края в изображений.\n",
    "\n",
    "**OpenCV** предоставляет функцию **cv2.filter2D()** для объединения ядра с изображением. В качестве примера мы попробуем усредняющий фильтр на изображении. Ядро усредняющего фильтра $5\\times5$ будет выглядеть так:\n",
    "\n",
    "${\\begin{equation*}\n",
    "\\ K = \\frac{1}{25} \\begin{pmatrix}\n",
    "1 & \\ 1 & \\ 1 & \\ 1 & \\ 1 \\\\\n",
    "1 & \\ 1 & \\ 1 & \\ 1 & \\ 1 \\\\\n",
    "1 & \\ 1 & \\ 1 & \\ 1 & \\ 1 \\\\\n",
    "1 & \\ 1 & \\ 1 & \\ 1 & \\ 1 \\\\\n",
    "1 & \\ 1 & \\ 1 & \\ 1 & \\ 1 \n",
    "\\end{pmatrix}\n",
    "\\end{equation*}}$\n",
    "\n",
    "Свертка выполняется согласно теоретическим примерам выше. Как результат мы получаем осредненные пиксели своими 25 соседями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:22:12.691792Z",
     "start_time": "2020-07-18T04:22:12.204037Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:22:26.353525Z",
     "start_time": "2020-07-18T04:22:26.343552Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/8_ka.jpg')\n",
    "# для отрисовки в pyplot\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:23:03.723309Z",
     "start_time": "2020-07-18T04:23:03.139716Z"
    }
   },
   "outputs": [],
   "source": [
    "kernel = np.ones((5, 5), dtype=np.float32)\n",
    "kernel /= np.sum(kernel)\n",
    "dst = cv2.filter2D(img.copy(), -1, kernel)\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(14, 10))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Исходное изображение')\n",
    "ax1.imshow(img)\n",
    "ax2.set_title('Результат свертки')\n",
    "ax2.imshow(dst);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фильтрация изображения\n",
    "Размытие изображения достигается путем свертывания изображения с помощью фильтра нижних частот. Это полезно для удаления шумов. Это фактически удаляет высокочастотный контент (например: шум, края) с изображения. Таким образом, края в этой операции немного размыты. (Ну, есть методы размытия, которые тоже не размывают края). **OpenCV** предоставляет в основном четыре типа техники размытия.\n",
    "\n",
    "Более подробное описание реализации фильтров OpenCV есть в [документации](https://docs.opencv.org/3.4.2/d4/d86/group__imgproc__filter.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Усреднение\n",
    "Это делается путем свертки изображения с помощью нормализованного прямоугольного фильтра. Он просто берет среднее значение всех пикселей в области ядра и заменяет центральный элемент. Это выполняется функцией **cv2.blur()** или **cv2.boxFilter()**. Для более подробной информации о ядре посмотрите в документации. Мы должны указать ширину и высоту ядра. Нормализованный блочный фильтр $3\\times3$ будет выглядеть следующим образом:\n",
    "\n",
    "${\\begin{equation*}K = \\frac{1}{9} \n",
    "{\\begin{pmatrix}\n",
    "1 & \\ 1 & \\ 1 \\\\ \n",
    "1 & \\ 1 & \\ 1 \\\\ \n",
    "1 & \\ 1 & \\ 1 \\end{pmatrix}}\n",
    "\\end{equation*}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:23:25.853405Z",
     "start_time": "2020-07-18T04:23:25.339705Z"
    }
   },
   "outputs": [],
   "source": [
    "blur = cv2.blur(img.copy(), (3, 3))\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(14, 10))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Исходное изображение')\n",
    "ax1.imshow(img)\n",
    "ax2.set_title('Результат свертки')\n",
    "ax2.imshow(blur);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Размытие по Гауссу\n",
    "При этом вместо box filter используется ядро Гаусса. Это делается с помощью функции **cv2.GaussianBlur()**. Мы должны указать ширину и высоту ядра, которые должны быть положительными и нечетными. Мы также должны указать стандартное отклонение в направлениях **X** и **Y**, **sigmaX** и **sigmaY** соответственно. Если указан только **sigmaX**, то **sigmaY** принимается так же, как **sigmaX**. Если оба даны в виде нулей, они рассчитываются по размеру ядра. Размытие по Гауссу очень эффективно для удаления гауссовского шума с изображения.\n",
    "\n",
    "Если вы хотите, вы можете создать ядро Гаусса с помощью функции **cv2.getGaussianKernel()**, а затем применить его с помощью **cv2.filter2D()**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гауссово окно? Что?\n",
    "\n",
    "Гауссова функция (гауссиан, гауссиана, функция Гаусса) — вещественная функция, описываемая следующей формулой:\n",
    "\n",
    "${\\displaystyle g\\left(x\\right)=a*exp{-{\\frac {(x-b)^{2}}{2c^{2}}}}}$\n",
    "\n",
    "Эта функция имеет широкое приминение в математической статистике. В таком случае параметры выражаются через среднеквадратичное отклонение ${\\displaystyle \\sigma }$ и математическое ожидание ${\\displaystyle \\mu }$ :\n",
    "\n",
    "${\\displaystyle a={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}}, {\\displaystyle b=\\mu }, {\\displaystyle c=\\sigma }$\n",
    "\n",
    "В двумерном виде она имеет такое представление:\n",
    "\n",
    "${\\displaystyle g(x,y)=A\\cdot exp{\\left(-\\left({\\frac {(x-x_{0})^{2}}{2\\sigma _{x}^{2}}}+{\\frac {(y-y_{0})^{2}}{2\\sigma _{y}^{2}}}\\right)\\right)}}$\n",
    "\n",
    "Стандартным статистическим способом мы определили ширину гауссовой формы в терминах $\\sigma$. Тем не менее, когда для сглаживания используется гауссиан, для сканеров характерно описывать ширину гауссиана с помощью другой связанной меры - полной ширины на половине максимума (FWHM).\n",
    "\n",
    "FWHM - ширина ядра, равная половине максимума высоты гауссианы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:43:48.417644Z",
     "start_time": "2020-07-18T04:43:48.194539Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def fwhm2sigma(fwhm):\n",
    "    return fwhm / np.sqrt(8 * np.log(2))\n",
    "FWHM = 1\n",
    "sigma = fwhm2sigma(FWHM)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "dx = 0.1\n",
    "dy = 0.1\n",
    "x = np.arange(-6, 6, dx)\n",
    "y = np.arange(-6, 6, dy)\n",
    "x2d, y2d = np.meshgrid(x, y)\n",
    "kernel_2d = np.exp(-(x2d ** 2 + y2d ** 2) / (2 * sigma ** 2))\n",
    "kernel_2d = kernel_2d / (2 * np.pi * sigma ** 2) # unit integral\n",
    "ax.plot_surface(x2d, y2d, kernel_2d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, можно представить, что окно гаусса представляет из себя проекцию функции на плоскость. Проиллюстрировано на картинке:\n",
    "<img src=\"img/gaus_win.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "\n",
    "Собственно, теперь перейдем к __threshold__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:43:51.549368Z",
     "start_time": "2020-07-18T04:43:50.545066Z"
    }
   },
   "outputs": [],
   "source": [
    "## Код для размытия по Гауссу: \n",
    "\n",
    "blur1 = cv2.GaussianBlur(img.copy(), (9,9), 0)\n",
    "blur2 = cv2.GaussianBlur(img.copy(), (9,9), 2)\n",
    "blur3 = cv2.GaussianBlur(img.copy(), (9,9), 5)\n",
    "\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 4, figsize=(22, 8))\n",
    "ax1, ax2, ax3, ax4 = m_axs\n",
    "\n",
    "ax1.set_title('Исходное изображение')\n",
    "ax1.imshow(img)\n",
    "ax2.set_title(r'Результат свертки $\\sigma_X = \\sigma_Y = 0$')\n",
    "ax2.imshow(blur1)\n",
    "ax3.set_title(r'Результат свертки $\\sigma_X = \\sigma_Y = 2$')\n",
    "ax3.imshow(blur2)\n",
    "ax4.set_title(r'Результат свертки $\\sigma_X = \\sigma_Y = 5$')\n",
    "ax4.imshow(blur3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Среднее размытие\n",
    "Здесь функция **cv2.medianBlur()** принимает медиану всех пикселей в области ядра, и центральный элемент заменяется этим медианным значением. Это очень эффективно против шума соли и перца на изображениях. Интересно, что в вышеупомянутых фильтрах центральным элементом является вновь вычисленное значение, которое может быть значением пикселя в изображении или новым значением. Но при медианном размытии центральный элемент всегда заменяется некоторым пиксельным значением на изображении. \n",
    "\n",
    "Эффективно снижает шум. Размер его ядра должен быть положительным нечетным целым числом.\n",
    "\n",
    "В этой демонстрации я добавил шум к нашему исходному изображению и применил медианное размытие. Проверьте результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:28:45.484276Z",
     "start_time": "2020-07-18T04:28:44.939359Z"
    }
   },
   "outputs": [],
   "source": [
    "## внесем шум\n",
    "noise_img = img.copy()\n",
    "mask = np.random.randint(0, 2, size=noise_img.shape).astype(np.bool)\n",
    "r = np.random.rand(*noise_img.shape) * np.max(noise_img)\n",
    "noise_img[mask] = r[mask]\n",
    "\n",
    "median = cv2.medianBlur(noise_img, 7)\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(14, 10))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Исходное изображение')\n",
    "ax1.imshow(noise_img)\n",
    "ax2.set_title('Результат свертки')\n",
    "ax2.imshow(median);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Двусторонняя фильтрация\n",
    "\n",
    "**cv2.bilateralFilter()** очень эффективно удаляет шум, сохраняя края острыми. Но операция медленнее по сравнению с другими фильтрами. Мы уже видели, что фильтр Гаусса берет окрестность вокруг пикселя и находит его средневзвешенное значение по Гауссу. Этот гауссов фильтр является функцией только одного пространства, то есть при фильтрации учитываются соседние пиксели. Он не учитывает, имеют ли пиксели почти одинаковую интенсивность. Он не учитывает, является ли пиксель краевым или нет. Таким образом, это стирает края, что мы не хотим делать.\n",
    "\n",
    "Двусторонний фильтр также принимает гауссов фильтр в пространстве, но еще один гауссов фильтр, который является функцией разности пикселей. Гауссова функция пространства обеспечивает размытие только соседних пикселей, а гауссова функция разности интенсивности - размытие только тех пикселей, интенсивность которых равна центральной. Таким образом, он сохраняет края, поскольку пиксели на краях будут сильно изменяться.\n",
    "\n",
    "**Рекомендации к фильтру:**\n",
    "\n",
    "* **Sigma values:** для простоты вы можете установить два значения сигмы одинаковыми. Если они маленькие ($<10$), фильтр не будет иметь большого эффекта, тогда как если они большие ($>150$), они будут иметь очень сильный эффект, делая изображение мультяшным.\n",
    "\n",
    "* **Размер фильтра:** Большие фильтры (d$>5$) очень медленные, поэтому рекомендуется использовать d $=5$ для приложений реального времени и, возможно, d $=9$ для автономных приложений, которые нуждаются в сильной фильтрации шума.\n",
    "\n",
    "Ниже приведены примеры использования двустороннего фильтра:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:32:23.505491Z",
     "start_time": "2020-07-18T04:32:22.987674Z"
    }
   },
   "outputs": [],
   "source": [
    "## внесем шум\n",
    "noise_img = img.copy()\n",
    "mask = np.random.randint(0, 2, size=noise_img.shape).astype(np.bool)\n",
    "r = np.random.rand(*noise_img.shape) * np.max(noise_img)\n",
    "noise_img[mask] = r[mask]\n",
    "\n",
    "bilateral = cv2.bilateralFilter(noise_img, d=5, sigmaColor=150, sigmaSpace=250)\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(14, 10))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Исходное изображение')\n",
    "ax1.imshow(noise_img)\n",
    "ax2.set_title('Результат свертки')\n",
    "ax2.imshow(bilateral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Морфологические трансформации\n",
    "Морфологические преобразования $-$ это простые операции, основанные на форме изображения. Обычно это выполняется на двоичных изображениях. Он требует двух входных данных, один из которых является нашим исходным изображением, второй называется структурирующим элементом или ядром, которое определяет характер операции. Два основных морфологических оператора - это **эрозия** и **растягивание**. Затем его разновидности, такие как открытие, закрытие, градиент и т. д.\n",
    "\n",
    "**Эрозия** (размывание/сужение) изображения обычно используется для избавления от случайных вкраплений на изображении. Идея состоит в том, что вкрапления при размывании устранятся, тогда как крупные и соответсвенно более визуально-значимые регионы остаются.\n",
    "\n",
    "**Растягивание** (расширение) же, по идее, так же должно устранять шум и способствовать объединению областей изображения, которые были разделены шумом, тенями.\n",
    "Применение же небольшого растягивания должно сплавить эти области в одну.\n",
    "\n",
    "Морфологические операции, чаще всего, применяются над двоичными изображениями, которые получаются после порогового преобразования (thresholding).\n",
    "***\n",
    "**Подробнее с математической морфологией можно ознакомиться по этим ссылка: [1](https://ru.wikipedia.org/wiki/%CC%E0%F2%E5%EC%E0%F2%E8%F7%E5%F1%EA%E0%FF_%EC%EE%F0%F4%EE%EB%EE%E3%E8%FF), [2](https://habr.com/post/113626/), [3](http://wiki.technicalvision.ru/index.php/%D0%9C%D0%BE%D1%80%D1%84%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D0%BE%D0%BF%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D0%B8_%D0%BD%D0%B0_%D0%B1%D0%B8%D0%BD%D0%B0%D1%80%D0%BD%D1%8B%D1%85_%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F%D1%85).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эрозия\n",
    "Основная идея эрозии похожа только на эрозию почвы, она размывает границы объекта переднего плана (всегда старайтесь, чтобы передний план оставался белым). Так, что это делает? Ядро скользит по изображению (как в **2D**-свертке). Пиксель в исходном изображении ($1$ или $0$) будет считаться $1$, только если все пиксели под ядром равны $1$, в противном случае он размыт (обнуляется).\n",
    "\n",
    "Итак, что происходит, так это то, что все пиксели вблизи границы будут отбрасываться в зависимости от размера ядра. Таким образом, толщина или размер объекта переднего плана уменьшается или просто белая область уменьшается на изображении. Это полезно для удаления небольших белых шумов (как мы видели в главе о цветовом пространстве), отсоединения двух связанных объектов и т. д.\n",
    "\n",
    "Здесь, в качестве примера, я бы использовал полное ядро $5\\times5$. Давайте посмотрим, как это работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:37:05.370917Z",
     "start_time": "2020-07-18T04:37:05.166383Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/j.png')\n",
    "\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "erosion = cv2.erode(img, kernel, iterations=1)\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(6, 4))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Исходное изображение')\n",
    "ax1.imshow(img, cmap='gray')\n",
    "ax2.set_title('Результат свертки')\n",
    "ax2.imshow(erosion, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Растягивание\n",
    "Это просто противоположность эрозии. Здесь пиксельный элемент равен «$1$», если хотя бы один пиксель под ядром равен «$1$». Таким образом, увеличивается белая область в изображении или увеличивается размер объекта переднего плана. Обычно в таких случаях, как удаление шума, за эрозией следует расширение. Потому что эрозия удаляет белые шумы, но также уменьшает наш объект. Таким образом, мы расширяем это. Поскольку шум исчез, они не вернутся, но наша площадь объекта увеличивается. Это также полезно при соединении сломанных частей объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:37:16.325115Z",
     "start_time": "2020-07-18T04:37:16.127552Z"
    }
   },
   "outputs": [],
   "source": [
    "dilation = cv2.dilate(img, kernel, iterations=1)\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(6, 4))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Исходное изображение')\n",
    "ax1.imshow(img, cmap='gray')\n",
    "ax2.set_title('Результат свертки')\n",
    "ax2.imshow(dilation, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Открытие\n",
    "Открытие - это просто еще одно название эрозии, за которой следует расширение. Это полезно для удаления шума, как мы объяснили выше. Здесь мы используем функцию **cv2.morphologyEx()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:37:36.926425Z",
     "start_time": "2020-07-18T04:37:36.714975Z"
    }
   },
   "outputs": [],
   "source": [
    "## внесем шум\n",
    "noise_img = img.copy()\n",
    "mask = np.random.randint(0, 2, size=noise_img.shape).astype(np.bool)\n",
    "r = np.ones(noise_img.shape) * 255\n",
    "noise_img[mask] = r[mask]\n",
    "\n",
    "opening = cv2.morphologyEx(noise_img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(6, 4))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Исходное изображение')\n",
    "ax1.imshow(noise_img, cmap='gray')\n",
    "ax2.set_title('Результат свертки')\n",
    "ax2.imshow(opening, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Закрытие\n",
    "Закрытие является обратным открытию, растягивание с последующей эрозией. Это полезно при закрытии небольших отверстий внутри объектов переднего плана или маленьких черных точек на объекте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:37:57.685731Z",
     "start_time": "2020-07-18T04:37:57.491983Z"
    }
   },
   "outputs": [],
   "source": [
    "## внесем шум\n",
    "noise_img = img.copy()\n",
    "mask = np.random.randint(0, 2, size=noise_img.shape).astype(np.bool)\n",
    "r = np.zeros(noise_img.shape)\n",
    "noise_img[mask] = r[mask]\n",
    "\n",
    "opening = cv2.morphologyEx(noise_img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(6, 4))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Исходное изображение')\n",
    "ax1.imshow(noise_img, cmap='gray')\n",
    "ax2.set_title('Результат свертки')\n",
    "ax2.imshow(opening, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Морфологический градиент\n",
    "Это разница между расширением и размыванием изображения.\n",
    "\n",
    "Результат будет выглядеть как контур объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:38:26.387209Z",
     "start_time": "2020-07-18T04:38:26.150848Z"
    }
   },
   "outputs": [],
   "source": [
    "gradient = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(6, 4))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Исходное изображение')\n",
    "ax1.imshow(img, cmap='gray')\n",
    "ax2.set_title('Результат свертки')\n",
    "ax2.imshow(gradient, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Thresholding\n",
    "Здесь все просто. Если значение пикселя больше порогового значения, ему присваивается одно значение (может быть белым), в противном случае ему присваивается другое значение (может быть черным). Используемая функция **cv2.threshold(src, thresh, maxval, type)**\n",
    "\n",
    "* **src** $-$ исходное изображение, которое должно быть изображением в градациях серого\n",
    "* **thresh** $-$ пороговое значение, которое используется для классификации значений пикселей\n",
    "* **maxval** $-$ представляет значение, которое будет дано, если значение пикселя больше (иногда меньше) порогового значения\n",
    "* **type** $-$ предоставляет различные стили порогового значения.\n",
    "\n",
    "Различные типы:\n",
    "1. cv2.THRESH_BINARY\n",
    "2. cv2.THRESH_BINARY_INV\n",
    "3. cv2.THRESH_TRUNC\n",
    "4. cv2.THRESH_TOZERO\n",
    "5. cv2.THRESH_TOZERO_INV\n",
    "\n",
    "На выходе функция возвращает два значения. Первый $-$ **retval**, которое будет объяснено позже. Второй $-$ **thresholded image**.\n",
    "\n",
    "Подробнее в [документации](https://docs.opencv.org/3.4.2/d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57).\n",
    "\n",
    "Рассмотрим разные типы threshold на примере градиента серого цвета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:45:34.999776Z",
     "start_time": "2020-07-18T04:45:33.914889Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/grad_grayscale.png')\n",
    "\n",
    "ret, thresh1 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "ret, thresh2 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "ret, thresh3 = cv2.threshold(img, 127, 255, cv2.THRESH_TRUNC)\n",
    "ret, thresh4 = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO)\n",
    "ret, thresh5 = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO_INV)\n",
    "\n",
    "titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1),plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Thresholding\n",
    "В предыдущем разделе мы использовали глобальное значение в качестве порогового значения. Но это может быть не хорошо во всех условиях, когда изображение имеет разные условия освещения в разных областях. В этом случае мы идем на адаптивный порог. При этом алгоритм вычисляет порог для небольших областей изображения. Таким образом, мы получаем разные пороговые значения для разных областей одного и того же изображения, и это дает нам лучшие результаты для изображений с разной освещенностью.\n",
    "\n",
    "Он имеет три «специальных» входных параметра и только один выходной аргумент.\n",
    "\n",
    "Адаптивный метод $-$ **cv2.adaptiveThreshold(src, maxValue, adaptiveMethod, thresholdType, blockSize, C)** - решает, как рассчитывается пороговое значение.\n",
    "\n",
    "* **thresholdType**:\n",
    "    * *cv2.ADAPTIVE_THRESH_MEAN_C*: пороговое значение является средним значением области соседства.\n",
    "\n",
    "    * *cv2.ADAPTIVE_THRESH_GAUSSIAN_C*: пороговое значение представляет собой взвешенную сумму значений окрестностей, где веса представляют собой гауссово окно.\n",
    "\n",
    "* **blockSize** $-$ определяет размер окна.\n",
    "\n",
    "* **C** $-$ это константа, которая вычитается из вычисленного среднего или взвешенного среднего.\n",
    "\n",
    "Ниже приведен фрагмент кода, сравнивающий глобальные пороги и адаптивные пороги для изображения с различным освещением:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:45:39.259136Z",
     "start_time": "2020-07-18T04:45:39.053406Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/sudoku.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img = cv2.medianBlur(img, 5)\n",
    "\n",
    "\n",
    "ret, th1 = cv2.threshold(img, 127, 255,cv2.THRESH_BINARY)\n",
    "th2 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "th3 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "\n",
    "titles = ['Original Image', 'Global Thresholding (v = 127)',\n",
    "            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\n",
    "images = [img, th1, th2, th3]\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1),plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бинаризация Оцу\n",
    "В первом разделе я говорил вам, что есть второй параметр retVal. Его использование происходит, когда мы идем на бинаризацию Оцу. Так что же это?\n",
    "\n",
    "В глобальном пороговом значении мы использовали произвольное значение для порогового значения, верно? Итак, как мы можем знать, что выбранное нами значение является хорошим или нет? Ответ, метод проб и ошибок. Но рассмотрим бимодальное изображение (проще говоря, бимодальное изображение - это изображение, гистограмма которого имеет два пика). Для этого изображения мы можем приблизительно принять значение в середине этих пиков в качестве порогового значения, верно? Это то, что делает бинаризация Оцу. Таким образом, простыми словами, он автоматически вычисляет пороговое значение из гистограммы изображения для бимодального изображения. (Для изображений, которые не являются бимодальными, бинаризация не будет точной.)\n",
    "\n",
    "Для этого используется наша функция **cv2.threshold()**, но передается дополнительный флаг *cv2.THRESH_OTSU*. Для порогового значения просто введите ноль. Затем алгоритм находит оптимальное пороговое значение и возвращает вас в качестве второго выхода retVal. Если пороговое значение Otsu не используется, **retVal** соответствует пороговому значению, которое вы использовали.\n",
    "\n",
    "Проверьте ниже пример. Входное изображение является шумным изображением. В первом случае я применил глобальный порог для значения $127$. Во втором случае я применил порог Оцу напрямую. В третьем случае я отфильтровал изображение с гауссовым ядром $5\\times5$, чтобы удалить шум, затем применил пороговое значение Оцу. Посмотрите, как фильтрация шума улучшает результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:48:15.913049Z",
     "start_time": "2020-07-18T04:48:14.738234Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/otsu_ex.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "noise_img = img.copy()\n",
    "mask = np.random.randint(0, 2, size=noise_img.shape).astype(np.bool)\n",
    "r = np.random.rand(*noise_img.shape) * np.max(noise_img)\n",
    "noise_img[mask] = r[mask]\n",
    "img = noise_img\n",
    "\n",
    "# global thresholding\n",
    "ret1, th1 = cv2.threshold(img, 170, 255, cv2.THRESH_BINARY)\n",
    "# Otsu's thresholding\n",
    "ret2, th2 = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "# Otsu's thresholding after Gaussian filtering\n",
    "blur = cv2.GaussianBlur(img, (5,5), 0)\n",
    "ret3, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "\n",
    "# plot all the images and their histograms\n",
    "images = [img, 0, th1,\n",
    "          img, 0, th2,\n",
    "          blur, 0, th3]\n",
    "ret = [ret1, ret2, ret3]\n",
    "titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n",
    "          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n",
    "          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i in range(3):\n",
    "    plt.subplot(3, 3, i*3 + 1), plt.imshow(images[i*3],'gray')\n",
    "    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n",
    "    \n",
    "    plt.subplot(3, 3, i*3 + 2), plt.hist(images[i*3].ravel(), 256)\n",
    "    plt.title(titles[i*3 + 1] + ', threshold =' + str(ret[i])), plt.xticks([]), plt.yticks([])\n",
    "    \n",
    "    plt.subplot(3, 3, i*3 + 3),plt.imshow(images[i*3 + 2],'gray')\n",
    "    plt.title(titles[i*3 + 2]), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Почему Гаусс и при чем тут aliasing?\n",
    "\n",
    "Размытие по Гауссу обычно используется при уменьшении размера изображения. При понижающей дискретизации изображения обычно применяют фильтр нижних частот к изображению перед повторной дискретизацией. Это делается для того, чтобы ложная высокочастотная информация не появлялась на изображении с пониженной дискретизацией (aliasing). Гауссовские размытия имеют хорошие свойства, такие как отсутствие острых краев. \n",
    "\n",
    "Проясним на примере:\n",
    "<table style=\"width:55%\"; table-layout:fixed;>\n",
    "  <tr>\n",
    "    <td><img src=\"img/alising_nan.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td><img src=\"img/alising.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Изображение без сжатия</td>\n",
    "    <td>Эффект alising от сжатия</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Pyramids\n",
    "\n",
    "Обычно мы работали с изображением постоянного размера. Но в некоторых случаях нам нужно работать с изображениями разного разрешения одного и того же изображения. Например, при поиске чего-либо на изображении, например лица, мы не уверены, в каком размере будет присутствовать объект на изображении. В этом случае нам нужно будет создать набор изображений с разным разрешением и выполнить поиск объекта по всем изображениям. Эти наборы изображений с разным разрешением называются пирамидами изображений (потому что, когда они хранятся в стопке с самым большим изображением внизу и самым маленьким изображением сверху, они выглядят как пирамида).\n",
    "\n",
    "Существует два вида пирамид изображений:\n",
    "1. Пирамида Гаусса\n",
    "2. Пирамиды Лапласа\n",
    "\n",
    "Более высокий уровень (низкое разрешение) в гауссовой пирамиде формируется путем удаления последовательных строк и столбцов в изображении нижнего уровня (более высокое разрешение). Затем каждый пиксель на более высоком уровне формируется вкладом из 5 пикселей на базовом уровне с гауссовыми весами. Таким образом, изображение $M \\times N$ становится изображением $M / 2 \\times N / 2$. То есть, площадь уменьшается до четверти первоначальной площади. Это называется Октава. Та же самая картина продолжается, когда мы идем вверх в пирамиде (то есть разрешение уменьшается). Аналогично, при расширении область становится 4 раза на каждом уровне. Мы можем найти гауссовы пирамиды, используя функции ```cv2.pyrDown()``` и ```cv2.pyrUp()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:48:21.746450Z",
     "start_time": "2020-07-18T04:48:21.727454Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/8_ka.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "h, w, _ = img.shape\n",
    "zero_back = np.zeros((h, w, 3), np.uint8)\n",
    "\n",
    "lower_reso = cv2.pyrDown(img)\n",
    "h, w, _ = lower_reso.shape\n",
    "zero_back[:h, :w, :] = lower_reso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:48:25.068809Z",
     "start_time": "2020-07-18T04:48:24.527065Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(1, 2, figsize=(12, 9))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Исходное изображение')\n",
    "ax1.imshow(img, cmap='gray')\n",
    "ax2.set_title('Пирамида')\n",
    "ax2.imshow(zero_back, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вы можете перейти вниз по пирамиде изображения с помощью функции ```cv2.pyrUp()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:48:35.408122Z",
     "start_time": "2020-07-18T04:48:35.392420Z"
    }
   },
   "outputs": [],
   "source": [
    "higher_reso = cv2.pyrUp(lower_reso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:48:36.920721Z",
     "start_time": "2020-07-18T04:48:36.476642Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(1, 2, figsize=(12, 9))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Исходное изображение')\n",
    "ax1.imshow(img, cmap='gray')\n",
    "ax2.set_title('Пирамида')\n",
    "ax2.imshow(higher_reso, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лапласовские пирамиды образованы из гауссовых пирамид. Для этого нет специальной функции. Изображения пирамиды Лапласа подобны только краевым изображениям. Большинство его элементов - нули. Они используются в сжатии изображений. Уровень в лапласовой пирамиде формируется разницей между этим текущим в гауссовой пирамиде и ее расширенной версией верхнего уровня в гауссовой пирамиде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:48:39.584115Z",
     "start_time": "2020-07-18T04:48:39.557415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Генерируем пирамиды Гаусса\n",
    "G = img.copy()\n",
    "gpA = [G]\n",
    "for i in range(6):\n",
    "    G = cv2.pyrDown(G)\n",
    "    gpA.append(G)\n",
    "    \n",
    "# Генерируем пирамиды Лапласа\n",
    "lpA = [gpA[5]]\n",
    "for i in range(5,0,-1):\n",
    "    GE = cv2.pyrUp(gpA[i])\n",
    "    h_ge, w_ge, _ = GE.shape\n",
    "    h_a, w_a, _ = gpA[i-1].shape\n",
    "    print(f'Разница между высотой и шириной:', abs(h_ge-h_a), abs(w_ge-w_a))\n",
    "    \n",
    "    # делаем падинг для согласования размеров изображений\n",
    "    gpA[i-1] = np.pad(gpA[i-1], pad_width=[(0, abs(h_ge-h_a)), (0, abs(w_ge-w_a)), (0, 0)], mode='edge')\n",
    "    print(f'Размеры изображений после падинга: {GE.shape, gpA[i-1].shape}\\n')\n",
    "    \n",
    "    L = cv2.subtract(gpA[i-1], GE)\n",
    "    lpA.append(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:48:43.927479Z",
     "start_time": "2020-07-18T04:48:43.133767Z"
    }
   },
   "outputs": [],
   "source": [
    "# Визуализация пирамид\n",
    "fig, m_axs = plt.subplots(2, 3, figsize=(24, 15))\n",
    "m_axs = m_axs.ravel(order='C')\n",
    "\n",
    "for i, ax in enumerate(m_axs):\n",
    "    ax.imshow(lpA[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending with pyramids\n",
    "\n",
    "Одним из применений пирамид является **смешивание изображений (blending)**. Например, при сшивании изображений вам нужно будет сложить два изображения вместе, но это может выглядеть не очень хорошо из-за разрыва между изображениями. В этом случае смешивание изображений с пирамидами обеспечивает плавное смешивание, не оставляя большого количества данных на изображениях. Одним из классических примеров этого является смешивание двух фруктов, апельсина и яблока. Теперь посмотрите на результат, чтобы понять, что я говорю:\n",
    "\n",
    "<img src=\"img/orapple.jpg\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "Пожалуйста, проверьте первую ссылку в дополнительных ресурсах, она имеет полную схематическую информацию о смешивании изображений, лапласовых пирамид и т. Д. Просто это делается следующим образом:\n",
    "\n",
    "* Загрузите два изображения яблока и апельсина\n",
    "* Найдите гауссовы пирамиды для яблок и апельсинов (в данном конкретном примере количество уровней равно 6)\n",
    "* Из гауссовых пирамид, найдите их лапласианские пирамиды\n",
    "* Теперь соедините левую половину яблока и правую половину апельсина на каждом уровне лапласианских пирамид\n",
    "* Наконец из этого совместного изображения пирамид, восстановить исходное изображение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практика\n",
    "\n",
    "Смешать два изображения (blending) с помощью пирамид. Примеры изображений загружены ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T04:58:55.072380Z",
     "start_time": "2020-07-18T04:58:54.753838Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_left = cv2.imread('img/cap_1.jpg')\n",
    "img_left = cv2.cvtColor(img_left, cv2.COLOR_BGR2RGB)\n",
    "img_right = cv2.imread('img/cap_2.jpg')\n",
    "img_right = cv2.cvtColor(img_right, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(f'Shape 1: {img_left.shape}, Shape 2: {img_right.shape}')\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(12, 9))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Изображение раз')\n",
    "ax1.imshow(img_left)\n",
    "ax2.set_title('Изображение два')\n",
    "ax2.imshow(img_right);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:06:26.836208Z",
     "start_time": "2020-07-18T05:06:26.829763Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_gp_lp(image, rg=6):\n",
    "    \n",
    "    # Генерируем пирамиды Гаусса\n",
    "    G = image.copy()\n",
    "    gpA = [G]\n",
    "    for i in range(rg):\n",
    "        G = cv2.pyrDown(G)\n",
    "        gpA.append(G)\n",
    "\n",
    "    # Генерируем пирамиды Лапласа\n",
    "    lpA = [gpA[rg - 1]]\n",
    "    for i in range(rg - 1, 0, -1):\n",
    "        GE = cv2.pyrUp(gpA[i])\n",
    "        h_ge, w_ge, _ = GE.shape\n",
    "        h_a, w_a, _ = gpA[i-1].shape\n",
    "\n",
    "        # делаем падинг для согласования размеров изображений\n",
    "        gpA[i-1] = np.pad(gpA[i-1], pad_width=[(0, abs(h_ge-h_a)), (0, abs(w_ge-w_a)), (0, 0)], mode='edge')\n",
    "\n",
    "        L = cv2.subtract(gpA[i-1], GE)\n",
    "        lpA.append(L)\n",
    "        \n",
    "    return gpA, lpA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:06:27.341719Z",
     "start_time": "2020-07-18T05:06:27.334694Z"
    }
   },
   "outputs": [],
   "source": [
    "rg = 5\n",
    "gpA, lpA = create_gp_lp(img_left, rg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:06:27.899279Z",
     "start_time": "2020-07-18T05:06:27.893278Z"
    }
   },
   "outputs": [],
   "source": [
    "gpB, lpB = create_gp_lp(img_right, rg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:06:28.309763Z",
     "start_time": "2020-07-18T05:06:28.303748Z"
    }
   },
   "outputs": [],
   "source": [
    "LS = []\n",
    "for la, lb in zip(lpA, lpB):\n",
    "    rows, cols, dpt = la.shape\n",
    "    ls = np.hstack((la[:, 0:cols//2], lb[:, cols//2:]))\n",
    "    LS.append(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:06:28.900221Z",
     "start_time": "2020-07-18T05:06:28.894946Z"
    }
   },
   "outputs": [],
   "source": [
    "[LS[i].shape for i in range(len(LS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T05:08:39.885840Z",
     "start_time": "2020-07-18T05:08:39.785063Z"
    }
   },
   "outputs": [],
   "source": [
    "# now reconstruct\n",
    "ls_ = LS[0]\n",
    "for i in range(1, rg):\n",
    "    ls_ = cv2.pyrUp(ls_)\n",
    "    ls_ = cv2.add(ls_, LS[i])\n",
    "\n",
    "# image with direct connecting each half\n",
    "real = np.hstack((ls_[:, :cols//2], ls_[:, cols//2:]))\n",
    "\n",
    "plt.imshow(real)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
